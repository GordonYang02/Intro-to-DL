{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxdEMdDv/orcTY9WT/NR2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GordonYang02/Intro-to-DL/blob/main/homework3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtFSmNpuWOBP",
        "outputId": "abd21c41-8c28-4082-b192-144d7f492b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'devNumber = torch.cuda.current_device()\\ndevName = torch.cuda.get_device_name(devNumber)\\n\\nprint(f\"Current device number is: {devNumber}\")\\nprint(f\"GPU name is: {devName}\")'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Problem 2\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import requests\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "'''devNumber = torch.cuda.current_device()\n",
        "devName = torch.cuda.get_device_name(devNumber)\n",
        "\n",
        "print(f\"Current device number is: {devNumber}\")\n",
        "print(f\"GPU name is: {devName}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Sample Data ---\n",
        "training_text = (\n",
        "    \"Next character prediction is a fundamental task in the field of natural language processing (NLP) \"\n",
        "    \"that involves predicting the next character in a sequence of text based on the characters that precede it. \"\n",
        "    \"This task is essential for various applications, including text auto-completion, spell checking, and even in the \"\n",
        "    \"development of sophisticated AI models capable of generating human-like text.\\n\"\n",
        "    \"At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given \"\n",
        "    \"sequence of text and predict which character is most likely to follow. These predictions are based on patterns and \"\n",
        "    \"relationships learned from large datasets of text during the training phase of the model.\\n\"\n",
        "    \"One of the most popular approaches to next character prediction involves the use of Recurrent Neural Networks (RNNs), \"\n",
        "    \"and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for \"\n",
        "    \"sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction \"\n",
        "    \"of the next character. LSTM networks enhance this capability by being able to remember long-term dependencies, making them \"\n",
        "    \"even more effective for next character prediction tasks.\\n\"\n",
        "    \"Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the \"\n",
        "    \"probability of each character's appearance following a sequence of characters. During this training process, the model \"\n",
        "    \"adjusts its parameters to minimize the difference between its predictions and the actual outcomes, thus improving its \"\n",
        "    \"predictive accuracy over time.\\n\"\n",
        "    \"Once trained, the model can be used to predict the next character in a given piece of text by considering the sequence of \"\n",
        "    \"characters that precede it. This can enhance user experience in text editing software, improve efficiency in coding environments \"\n",
        "    \"with auto-completion features, and enable more natural interactions with AI-based chatbots and virtual assistants.\\n\"\n",
        "    \"In summary, next character prediction plays a crucial role in enhancing the capabilities of various NLP applications, making \"\n",
        "    \"text-based interactions more efficient, accurate, and human-like. Through the use of advanced machine learning models like \"\n",
        "    \"RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future of text-based technology.\"\n",
        ")\n",
        "\n",
        "# --- Create Character Mappings ---\n",
        "all_chars = sorted(set(training_text))\n",
        "idx_to_char = {idx: ch for idx, ch in enumerate(all_chars)}\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(all_chars)}\n",
        "\n",
        "# --- Dataset Creation ---\n",
        "def create_dataset(seq_len, text, mapping):\n",
        "    \"\"\"Convert text into input sequences and target characters.\"\"\"\n",
        "    inputs, targets = [], []\n",
        "    for pos in range(len(text) - seq_len):\n",
        "        chunk = text[pos: pos + seq_len]\n",
        "        target_char = text[pos + seq_len]\n",
        "        inputs.append([mapping[c] for c in chunk])\n",
        "        targets.append(mapping[target_char])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, num_classes, cell_variant=\"RNN\"):\n",
        "        super(CharPredictor, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "        cell_variant = cell_variant.upper()\n",
        "        if cell_variant == \"RNN\":\n",
        "            self.cell = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
        "        elif cell_variant == \"LSTM\":\n",
        "            self.cell = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        elif cell_variant == \"GRU\":\n",
        "            self.cell = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"cell_variant must be one of: RNN, LSTM, GRU\")\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        # Embed input indices to vectors\n",
        "        embedded_seq = self.embed(seq)\n",
        "        # Process sequence through the recurrent cell\n",
        "        rnn_out, _ = self.cell(embedded_seq)\n",
        "        # Use the output from the final time step\n",
        "        final_step = rnn_out[:, -1, :]\n",
        "        return self.classifier(final_step)\n",
        "\n",
        "# --- Training & Evaluation ---\n",
        "def run_training(model, train_data, valid_data, num_epochs, lr):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    X_train, y_train = train_data\n",
        "    X_valid, y_valid = valid_data\n",
        "\n",
        "    t0 = time.time()\n",
        "    for ep in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_train)\n",
        "        loss = criterion(logits, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(X_valid)\n",
        "            val_loss = criterion(val_logits, y_valid)\n",
        "            _, preds = torch.max(val_logits, dim=1)\n",
        "            accuracy = (preds == y_valid).float().mean()\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            print(f\"Epoch {ep+1}: Train Loss={loss.item():.4f} | Val Loss={val_loss.item():.4f} | Val Acc={accuracy.item():.4f}\")\n",
        "    elapsed = time.time() - t0\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    return loss.item(), accuracy.item(), elapsed, total_params\n",
        "\n",
        "# --- Main Experiment Loop ---\n",
        "if __name__ == '__main__':\n",
        "    # Experiment settings\n",
        "    seq_options = [10, 20, 30]\n",
        "    cell_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
        "    hidden_dim = 128\n",
        "    num_epochs = 100\n",
        "    lr = 0.005\n",
        "\n",
        "    experiment_results = []\n",
        "\n",
        "    for cell in cell_types:\n",
        "        print(f\"\\n=== Experiment: {cell} Model ===\")\n",
        "        for seq_len in seq_options:\n",
        "            print(f\"\\n--- Sequence Length: {seq_len} ---\")\n",
        "            # Generate full dataset for current sequence length\n",
        "            X_all, y_all = create_dataset(seq_len, training_text, char_to_idx)\n",
        "            # Split into training and validation sets\n",
        "            X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
        "                X_all, y_all, test_size=0.2, random_state=42\n",
        "            )\n",
        "            # Convert numpy arrays to PyTorch tensors\n",
        "            X_train = torch.tensor(X_train_np, dtype=torch.long)\n",
        "            y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
        "            X_val = torch.tensor(X_val_np, dtype=torch.long)\n",
        "            y_val = torch.tensor(y_val_np, dtype=torch.long)\n",
        "\n",
        "            # Initialize the model\n",
        "            model_inst = CharPredictor(\n",
        "                vocab_size=len(all_chars),\n",
        "                hidden_dim=hidden_dim,\n",
        "                num_classes=len(all_chars),\n",
        "                cell_variant=cell\n",
        "            )\n",
        "            # Train and evaluate the model\n",
        "            final_loss, final_acc, elapsed_time, param_count = run_training(\n",
        "                model_inst, (X_train, y_train), (X_val, y_val), num_epochs, lr\n",
        "            )\n",
        "            experiment_results.append({\n",
        "                'Cell Type': cell,\n",
        "                'Sequence Length': seq_len,\n",
        "                'Final Loss': final_loss,\n",
        "                'Validation Accuracy': final_acc,\n",
        "                'Time (s)': elapsed_time,\n",
        "                'Parameter Count': param_count\n",
        "            })\n",
        "\n",
        "   # --- Print Summary ---\n",
        "print(\"\\n=== Summary of Results ===\")\n",
        "for res in experiment_results:\n",
        "    print(f\"{res['Cell Type']} | Seq Len: {res['Sequence Length']} | Loss: {res['Final Loss']:.4f} | \"\n",
        "          f\"Val Acc: {res['Validation Accuracy']:.4f} | Time: {res['Time (s)']:.2f}s | Params: {res['Parameter Count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jDoMXhNw8ws",
        "outputId": "74b183af-458d-4107-ae8d-1871006b3baa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Experiment: RNN Model ===\n",
            "\n",
            "--- Sequence Length: 10 ---\n",
            "Epoch 10: Train Loss=2.2529 | Val Loss=2.2937 | Val Acc=0.3866\n",
            "Epoch 20: Train Loss=1.7820 | Val Loss=2.0591 | Val Acc=0.4286\n",
            "Epoch 30: Train Loss=1.4257 | Val Loss=1.9322 | Val Acc=0.4811\n",
            "Epoch 40: Train Loss=1.1156 | Val Loss=1.8887 | Val Acc=0.4979\n",
            "Epoch 50: Train Loss=0.8352 | Val Loss=1.8972 | Val Acc=0.5336\n",
            "Epoch 60: Train Loss=0.6036 | Val Loss=1.9626 | Val Acc=0.5567\n",
            "Epoch 70: Train Loss=0.4017 | Val Loss=2.0809 | Val Acc=0.5609\n",
            "Epoch 80: Train Loss=0.2560 | Val Loss=2.2224 | Val Acc=0.5483\n",
            "Epoch 90: Train Loss=0.1584 | Val Loss=2.3785 | Val Acc=0.5399\n",
            "Epoch 100: Train Loss=0.1054 | Val Loss=2.5253 | Val Acc=0.5441\n",
            "\n",
            "--- Sequence Length: 20 ---\n",
            "Epoch 10: Train Loss=2.2658 | Val Loss=2.3164 | Val Acc=0.3734\n",
            "Epoch 20: Train Loss=1.8031 | Val Loss=2.0673 | Val Acc=0.4473\n",
            "Epoch 30: Train Loss=1.4424 | Val Loss=1.9782 | Val Acc=0.4873\n",
            "Epoch 40: Train Loss=1.1247 | Val Loss=1.9556 | Val Acc=0.4852\n",
            "Epoch 50: Train Loss=0.8389 | Val Loss=1.9729 | Val Acc=0.5021\n",
            "Epoch 60: Train Loss=0.6185 | Val Loss=2.0391 | Val Acc=0.4979\n",
            "Epoch 70: Train Loss=0.4161 | Val Loss=2.1500 | Val Acc=0.5042\n",
            "Epoch 80: Train Loss=0.2709 | Val Loss=2.2741 | Val Acc=0.5063\n",
            "Epoch 90: Train Loss=0.1785 | Val Loss=2.4199 | Val Acc=0.5105\n",
            "Epoch 100: Train Loss=0.1120 | Val Loss=2.5062 | Val Acc=0.5084\n",
            "\n",
            "--- Sequence Length: 30 ---\n",
            "Epoch 10: Train Loss=2.2479 | Val Loss=2.3847 | Val Acc=0.3326\n",
            "Epoch 20: Train Loss=1.8007 | Val Loss=2.1827 | Val Acc=0.4047\n",
            "Epoch 30: Train Loss=1.4461 | Val Loss=2.0789 | Val Acc=0.4513\n",
            "Epoch 40: Train Loss=1.1300 | Val Loss=2.0243 | Val Acc=0.4788\n",
            "Epoch 50: Train Loss=0.8418 | Val Loss=2.0394 | Val Acc=0.4979\n",
            "Epoch 60: Train Loss=0.6180 | Val Loss=2.1124 | Val Acc=0.5000\n",
            "Epoch 70: Train Loss=0.4056 | Val Loss=2.2325 | Val Acc=0.5021\n",
            "Epoch 80: Train Loss=0.2563 | Val Loss=2.3750 | Val Acc=0.5042\n",
            "Epoch 90: Train Loss=0.1598 | Val Loss=2.5014 | Val Acc=0.4915\n",
            "Epoch 100: Train Loss=0.1021 | Val Loss=2.6130 | Val Acc=0.4979\n",
            "\n",
            "=== Experiment: LSTM Model ===\n",
            "\n",
            "--- Sequence Length: 10 ---\n",
            "Epoch 10: Train Loss=2.5453 | Val Loss=2.4692 | Val Acc=0.3172\n",
            "Epoch 20: Train Loss=2.0468 | Val Loss=2.1511 | Val Acc=0.4328\n",
            "Epoch 30: Train Loss=1.6596 | Val Loss=1.9881 | Val Acc=0.4580\n",
            "Epoch 40: Train Loss=1.3074 | Val Loss=1.9039 | Val Acc=0.4937\n",
            "Epoch 50: Train Loss=1.0009 | Val Loss=1.8814 | Val Acc=0.5126\n",
            "Epoch 60: Train Loss=0.7264 | Val Loss=1.9208 | Val Acc=0.5042\n",
            "Epoch 70: Train Loss=0.4939 | Val Loss=1.9763 | Val Acc=0.5210\n",
            "Epoch 80: Train Loss=0.3117 | Val Loss=2.0555 | Val Acc=0.5105\n",
            "Epoch 90: Train Loss=0.1918 | Val Loss=2.1446 | Val Acc=0.5126\n",
            "Epoch 100: Train Loss=0.1244 | Val Loss=2.2712 | Val Acc=0.4748\n",
            "\n",
            "--- Sequence Length: 20 ---\n",
            "Epoch 10: Train Loss=2.5517 | Val Loss=2.5153 | Val Acc=0.3080\n",
            "Epoch 20: Train Loss=2.0658 | Val Loss=2.1975 | Val Acc=0.3713\n",
            "Epoch 30: Train Loss=1.6839 | Val Loss=2.0070 | Val Acc=0.4641\n",
            "Epoch 40: Train Loss=1.3297 | Val Loss=1.9020 | Val Acc=0.4747\n",
            "Epoch 50: Train Loss=1.0121 | Val Loss=1.8624 | Val Acc=0.5000\n",
            "Epoch 60: Train Loss=0.7367 | Val Loss=1.8900 | Val Acc=0.5253\n",
            "Epoch 70: Train Loss=0.5084 | Val Loss=1.9594 | Val Acc=0.5169\n",
            "Epoch 80: Train Loss=0.3381 | Val Loss=2.0482 | Val Acc=0.5190\n",
            "Epoch 90: Train Loss=0.2081 | Val Loss=2.1106 | Val Acc=0.5253\n",
            "Epoch 100: Train Loss=0.1283 | Val Loss=2.2002 | Val Acc=0.5063\n",
            "\n",
            "--- Sequence Length: 30 ---\n",
            "Epoch 10: Train Loss=2.5754 | Val Loss=2.6085 | Val Acc=0.2860\n",
            "Epoch 20: Train Loss=2.0975 | Val Loss=2.2778 | Val Acc=0.3919\n",
            "Epoch 30: Train Loss=1.7275 | Val Loss=2.0697 | Val Acc=0.4364\n",
            "Epoch 40: Train Loss=1.3830 | Val Loss=1.9787 | Val Acc=0.4831\n",
            "Epoch 50: Train Loss=1.0647 | Val Loss=1.9506 | Val Acc=0.5064\n",
            "Epoch 60: Train Loss=0.7735 | Val Loss=1.9598 | Val Acc=0.5042\n",
            "Epoch 70: Train Loss=0.5377 | Val Loss=2.0153 | Val Acc=0.5233\n",
            "Epoch 80: Train Loss=0.3578 | Val Loss=2.0845 | Val Acc=0.5233\n",
            "Epoch 90: Train Loss=0.2356 | Val Loss=2.1764 | Val Acc=0.5233\n",
            "Epoch 100: Train Loss=0.1484 | Val Loss=2.2615 | Val Acc=0.5254\n",
            "\n",
            "=== Experiment: GRU Model ===\n",
            "\n",
            "--- Sequence Length: 10 ---\n",
            "Epoch 10: Train Loss=2.3907 | Val Loss=2.3667 | Val Acc=0.3592\n",
            "Epoch 20: Train Loss=1.8653 | Val Loss=2.0724 | Val Acc=0.4307\n",
            "Epoch 30: Train Loss=1.4629 | Val Loss=1.9194 | Val Acc=0.4748\n",
            "Epoch 40: Train Loss=1.0996 | Val Loss=1.8634 | Val Acc=0.5105\n",
            "Epoch 50: Train Loss=0.7756 | Val Loss=1.8676 | Val Acc=0.5231\n",
            "Epoch 60: Train Loss=0.5059 | Val Loss=1.9564 | Val Acc=0.5252\n",
            "Epoch 70: Train Loss=0.3071 | Val Loss=2.0890 | Val Acc=0.5147\n",
            "Epoch 80: Train Loss=0.1775 | Val Loss=2.1962 | Val Acc=0.5126\n",
            "Epoch 90: Train Loss=0.1073 | Val Loss=2.3211 | Val Acc=0.5168\n",
            "Epoch 100: Train Loss=0.0747 | Val Loss=2.4082 | Val Acc=0.5105\n",
            "\n",
            "--- Sequence Length: 20 ---\n",
            "Epoch 10: Train Loss=2.3819 | Val Loss=2.3820 | Val Acc=0.3544\n",
            "Epoch 20: Train Loss=1.8748 | Val Loss=2.0993 | Val Acc=0.4409\n",
            "Epoch 30: Train Loss=1.4668 | Val Loss=1.9370 | Val Acc=0.4958\n",
            "Epoch 40: Train Loss=1.0958 | Val Loss=1.8685 | Val Acc=0.5338\n",
            "Epoch 50: Train Loss=0.7645 | Val Loss=1.8889 | Val Acc=0.5443\n",
            "Epoch 60: Train Loss=0.4872 | Val Loss=1.9600 | Val Acc=0.5464\n",
            "Epoch 70: Train Loss=0.2850 | Val Loss=2.1163 | Val Acc=0.5401\n",
            "Epoch 80: Train Loss=0.1581 | Val Loss=2.2788 | Val Acc=0.5338\n",
            "Epoch 90: Train Loss=0.0882 | Val Loss=2.4191 | Val Acc=0.5295\n",
            "Epoch 100: Train Loss=0.0566 | Val Loss=2.5305 | Val Acc=0.5127\n",
            "\n",
            "--- Sequence Length: 30 ---\n",
            "Epoch 10: Train Loss=2.3734 | Val Loss=2.4767 | Val Acc=0.3432\n",
            "Epoch 20: Train Loss=1.8535 | Val Loss=2.1276 | Val Acc=0.4428\n",
            "Epoch 30: Train Loss=1.4490 | Val Loss=1.9980 | Val Acc=0.4513\n",
            "Epoch 40: Train Loss=1.0789 | Val Loss=1.9852 | Val Acc=0.4725\n",
            "Epoch 50: Train Loss=0.7455 | Val Loss=2.0324 | Val Acc=0.5000\n",
            "Epoch 60: Train Loss=0.4744 | Val Loss=2.1170 | Val Acc=0.5106\n",
            "Epoch 70: Train Loss=0.2781 | Val Loss=2.2470 | Val Acc=0.5148\n",
            "Epoch 80: Train Loss=0.1610 | Val Loss=2.4066 | Val Acc=0.5148\n",
            "Epoch 90: Train Loss=0.0936 | Val Loss=2.5556 | Val Acc=0.5106\n",
            "Epoch 100: Train Loss=0.0576 | Val Loss=2.6688 | Val Acc=0.5085\n",
            "\n",
            "=== Summary of Results ===\n",
            "RNN | Seq Len: 10 | Loss: 0.1054 | Val Acc: 0.5441 | Time: 1.78s | Params: 44589\n",
            "RNN | Seq Len: 20 | Loss: 0.1120 | Val Acc: 0.5084 | Time: 3.27s | Params: 44589\n",
            "RNN | Seq Len: 30 | Loss: 0.1021 | Val Acc: 0.4979 | Time: 4.82s | Params: 44589\n",
            "LSTM | Seq Len: 10 | Loss: 0.1244 | Val Acc: 0.4748 | Time: 3.11s | Params: 143661\n",
            "LSTM | Seq Len: 20 | Loss: 0.1283 | Val Acc: 0.5063 | Time: 5.69s | Params: 143661\n",
            "LSTM | Seq Len: 30 | Loss: 0.1484 | Val Acc: 0.5254 | Time: 9.17s | Params: 143661\n",
            "GRU | Seq Len: 10 | Loss: 0.0747 | Val Acc: 0.5105 | Time: 4.63s | Params: 110637\n",
            "GRU | Seq Len: 20 | Loss: 0.0566 | Val Acc: 0.5127 | Time: 9.02s | Params: 110637\n",
            "GRU | Seq Len: 30 | Loss: 0.0576 | Val Acc: 0.5085 | Time: 13.45s | Params: 110637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import requests\n",
        "\n",
        "# --- Data Acquisition ---\n",
        "def fetch_shakespeare(url=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise RuntimeError(\"Failed to download the dataset.\")\n",
        "\n",
        "# --- Data Preparation ---\n",
        "def prepare_shakespeare_data(seq_len, text):\n",
        "    # Create a sorted set of characters and mapping dictionaries\n",
        "    unique_chars = sorted(set(text))\n",
        "    char2idx = {ch: idx for idx, ch in enumerate(unique_chars)}\n",
        "    idx2char = {idx: ch for idx, ch in enumerate(unique_chars)}\n",
        "\n",
        "    # Convert the text to a list of integer indices\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "\n",
        "    # Create sequences and corresponding targets\n",
        "    seq_list, target_list = [], []\n",
        "    for pos in range(len(encoded) - seq_len):\n",
        "        seq_list.append(encoded[pos:pos + seq_len])\n",
        "        target_list.append(encoded[pos + seq_len])\n",
        "\n",
        "    # Return tensors along with the mappings\n",
        "    return (torch.tensor(seq_list, dtype=torch.long),\n",
        "            torch.tensor(target_list, dtype=torch.long),\n",
        "            char2idx, idx2char)\n",
        "\n",
        "# --- Custom Dataset ---\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, out_dim, cell_type=\"LSTM\", num_layers=1):\n",
        "        super(CharRNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
        "\n",
        "        cell_type = cell_type.upper()\n",
        "        if cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"cell_type must be either 'LSTM' or 'GRU'\")\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length)\n",
        "        x_embed = self.embed(x)  # (batch_size, sequence_length, hidden_dim)\n",
        "        rnn_out, _ = self.rnn(x_embed)\n",
        "        # Use the output from the final time step\n",
        "        final_output = rnn_out[:, -1, :]\n",
        "        logits = self.fc(final_output)\n",
        "        return logits\n",
        "\n",
        "# --- Training and Evaluation ---\n",
        "def train_model(model, train_loader, valid_loader, epochs, lr, device):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(batch_x)\n",
        "            loss = loss_fn(preds, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss_total = 0.0\n",
        "        correct = 0\n",
        "        count = 0\n",
        "        with torch.no_grad():\n",
        "            for v_x, v_y in valid_loader:\n",
        "                v_x, v_y = v_x.to(device), v_y.to(device)\n",
        "                v_preds = model(v_x)\n",
        "                loss_val = loss_fn(v_preds, v_y)\n",
        "                val_loss_total += loss_val.item()\n",
        "                _, predicted_labels = torch.max(v_preds, dim=1)\n",
        "                correct += (predicted_labels == v_y).sum().item()\n",
        "                count += v_y.size(0)\n",
        "        avg_val_loss = val_loss_total / len(valid_loader)\n",
        "        val_accuracy = correct / count\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    return avg_train_loss, val_accuracy, total_time, param_count\n",
        "\n",
        "# --- Main Experiment ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Download the tiny Shakespeare dataset\n",
        "    shakespeare_text = fetch_shakespeare()\n",
        "\n",
        "    # Hyperparameters and settings\n",
        "    seq_lengths = [20, 30, 50]  # Include sequence length 50 as requested\n",
        "    cell_variants = [\"LSTM\", \"GRU\"]\n",
        "    hidden_dim = 128\n",
        "    num_epochs = 50\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.005\n",
        "    num_layers = 1  # Can adjust to experiment with deeper RNNs\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Loop over sequence lengths and model types\n",
        "    for seq_len in seq_lengths:\n",
        "        print(f\"\\n=== Processing Sequence Length: {seq_len} ===\")\n",
        "        X_data, y_data, char2idx, idx2char = prepare_shakespeare_data(seq_len, shakespeare_text)\n",
        "        dataset = ShakespeareDataset(X_data, y_data)\n",
        "\n",
        "        # Split dataset into training and validation subsets\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        valid_size = len(dataset) - train_size\n",
        "        train_set, valid_set = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        for cell in cell_variants:\n",
        "            print(f\"\\n--- Training {cell} model with sequence length {seq_len} ---\\n\")\n",
        "            model = CharRNNModel(\n",
        "                vocab_size=len(char2idx),\n",
        "                hidden_dim=hidden_dim,\n",
        "                out_dim=len(char2idx),\n",
        "                cell_type=cell,\n",
        "                num_layers=num_layers\n",
        "            )\n",
        "            train_loss, val_acc, train_time, model_params = train_model(\n",
        "                model, train_loader, valid_loader, num_epochs, learning_rate, device\n",
        "            )\n",
        "            results.append({\n",
        "                \"Cell Type\": cell,\n",
        "                \"Seq Length\": seq_len,\n",
        "                \"Train Loss\": train_loss,\n",
        "                \"Val Acc\": val_acc,\n",
        "                \"Time (s)\": train_time,\n",
        "                \"Model Size\": model_params\n",
        "            })\n",
        "\n",
        "    # --- Summary of Results ---\n",
        "print(\"\\n=== Final Model Comparison ===\")\n",
        "for res in results:\n",
        "    print(f\"{res['Cell Type']} | Seq Len: {res['Seq Length']} | Loss: {res['Train Loss']:.4f} | Val Acc: {res['Val Acc']:.4f} | Time: {res['Time (s)']:.2f}s | Params: {res['Model Size']}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgvdt2T4w8zf",
        "outputId": "50b45688-0712-460d-bf28-ea06494dc203"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing Sequence Length: 20 ===\n",
            "\n",
            "--- Training LSTM model with sequence length 20 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.5437 | Val Loss: 1.5639 | Val Acc: 0.5265\n",
            "Epoch 20: Train Loss: 1.5946 | Val Loss: 1.6108 | Val Acc: 0.5153\n",
            "Epoch 30: Train Loss: 1.6395 | Val Loss: 1.6616 | Val Acc: 0.5025\n",
            "Epoch 40: Train Loss: 1.6615 | Val Loss: 1.6788 | Val Acc: 0.4995\n",
            "Epoch 50: Train Loss: 1.6783 | Val Loss: 1.6913 | Val Acc: 0.4915\n",
            "\n",
            "--- Training GRU model with sequence length 20 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.8482 | Val Loss: 1.8565 | Val Acc: 0.4568\n",
            "Epoch 20: Train Loss: 1.8582 | Val Loss: 1.8808 | Val Acc: 0.4480\n",
            "Epoch 30: Train Loss: 1.8616 | Val Loss: 1.8535 | Val Acc: 0.4585\n",
            "Epoch 40: Train Loss: 1.8523 | Val Loss: 1.8584 | Val Acc: 0.4589\n",
            "Epoch 50: Train Loss: 1.8428 | Val Loss: 1.8611 | Val Acc: 0.4554\n",
            "\n",
            "=== Processing Sequence Length: 30 ===\n",
            "\n",
            "--- Training LSTM model with sequence length 30 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.5261 | Val Loss: 1.5479 | Val Acc: 0.5354\n",
            "Epoch 20: Train Loss: 1.5768 | Val Loss: 1.5974 | Val Acc: 0.5202\n",
            "Epoch 30: Train Loss: 1.6228 | Val Loss: 1.6451 | Val Acc: 0.5075\n",
            "Epoch 40: Train Loss: 1.6587 | Val Loss: 1.6852 | Val Acc: 0.4946\n",
            "Epoch 50: Train Loss: 1.6744 | Val Loss: 1.6904 | Val Acc: 0.4964\n",
            "\n",
            "--- Training GRU model with sequence length 30 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.8255 | Val Loss: 1.8469 | Val Acc: 0.4646\n",
            "Epoch 20: Train Loss: 1.8331 | Val Loss: 1.8394 | Val Acc: 0.4626\n",
            "Epoch 30: Train Loss: 1.8307 | Val Loss: 1.8408 | Val Acc: 0.4597\n",
            "Epoch 40: Train Loss: 1.8509 | Val Loss: 1.8946 | Val Acc: 0.4499\n",
            "Epoch 50: Train Loss: 1.8698 | Val Loss: 1.8844 | Val Acc: 0.4541\n",
            "\n",
            "=== Processing Sequence Length: 50 ===\n",
            "\n",
            "--- Training LSTM model with sequence length 50 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.5335 | Val Loss: 1.5505 | Val Acc: 0.5332\n",
            "Epoch 20: Train Loss: 1.5855 | Val Loss: 1.6024 | Val Acc: 0.5199\n",
            "Epoch 30: Train Loss: 1.6517 | Val Loss: 1.6706 | Val Acc: 0.5030\n",
            "Epoch 40: Train Loss: 1.6734 | Val Loss: 1.6831 | Val Acc: 0.5009\n",
            "Epoch 50: Train Loss: 1.6713 | Val Loss: 1.6870 | Val Acc: 0.4991\n",
            "\n",
            "--- Training GRU model with sequence length 50 ---\n",
            "\n",
            "Epoch 10: Train Loss: 1.8100 | Val Loss: 1.8280 | Val Acc: 0.4671\n",
            "Epoch 20: Train Loss: 1.8232 | Val Loss: 1.8274 | Val Acc: 0.4664\n",
            "Epoch 30: Train Loss: 1.8394 | Val Loss: 1.8377 | Val Acc: 0.4601\n",
            "Epoch 40: Train Loss: 1.8662 | Val Loss: 1.8543 | Val Acc: 0.4596\n",
            "Epoch 50: Train Loss: 1.8583 | Val Loss: 1.8657 | Val Acc: 0.4499\n",
            "\n",
            "=== Final Model Comparison ===\n",
            "LSTM | Seq Len: 20 | Loss: 1.6783 | Val Acc: 0.4915 | Time: 696.32s | Params: 148801\n",
            "GRU | Seq Len: 20 | Loss: 1.8428 | Val Acc: 0.4554 | Time: 642.47s | Params: 115777\n",
            "LSTM | Seq Len: 30 | Loss: 1.6744 | Val Acc: 0.4964 | Time: 748.03s | Params: 148801\n",
            "GRU | Seq Len: 30 | Loss: 1.8698 | Val Acc: 0.4541 | Time: 719.88s | Params: 115777\n",
            "LSTM | Seq Len: 50 | Loss: 1.6713 | Val Acc: 0.4991 | Time: 785.96s | Params: 148801\n",
            "GRU | Seq Len: 50 | Loss: 1.8583 | Val Acc: 0.4499 | Time: 733.70s | Params: 115777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Problem 2 Adjust hyperparameters (fully connected network, number of hidden layers, and the number of hidden states)\"\"\"\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Download the Tiny Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # the whole text\n",
        "\n",
        "# Function to convert text into sequences of a given length\n",
        "def get_data(seq_len):\n",
        "    # Get unique characters and build mappings\n",
        "    chars = sorted(set(text))\n",
        "    char2idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "    # Encode the text into numbers\n",
        "    encoded = [char2idx[c] for c in text]\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(encoded) - seq_len):\n",
        "        X.append(encoded[i:i+seq_len])\n",
        "        y.append(encoded[i+seq_len])\n",
        "    return torch.tensor(X, dtype=torch.long), torch.tensor(y, dtype=torch.long), char2idx, idx2char\n",
        "\n",
        "# Create a simple Dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Define a simple RNN model using either LSTM or GRU\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, model_type=\"LSTM\"):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
        "        if model_type.upper() == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)           # shape: [batch, seq_len, hidden_size]\n",
        "        out, _ = self.rnn(x)      # shape: [batch, seq_len, hidden_size]\n",
        "        out = self.fc(out[:, -1, :])  # use last time step\n",
        "        return out\n",
        "\n",
        "# Training loop function\n",
        "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "    start = time.time()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                outputs = model(xb)\n",
        "                l = criterion(outputs, yb)\n",
        "                val_loss += l.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        acc = correct / total\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            print(f\"Epoch {ep+1}: Train {avg_loss:.4f}, Val {avg_val_loss:.4f}, Acc {acc:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    return avg_loss, acc, total_time, num_params\n",
        "\n",
        "# Settings\n",
        "seq_lengths = [20, 30]   # you can add 50 if needed\n",
        "model_types = [\"LSTM\", \"GRU\"]\n",
        "hidden_size = 64\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "results = []\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Main loop for experiments\n",
        "for sl in seq_lengths:\n",
        "    print(f\"\\nProcessing sequence length: {sl}\")\n",
        "    X, y, c2i, i2c = get_data(sl)\n",
        "    dataset = MyDataset(X, y)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for typ in model_types:\n",
        "        print(f\"Training {typ} model with seq_len {sl}\")\n",
        "        model = SimpleRNN(len(c2i), hidden_size, model_type=typ)\n",
        "        tr_loss, val_acc, t_time, m_size = train_model(model, train_loader, val_loader, epochs, lr, device)\n",
        "        results.append((typ, sl, tr_loss, val_acc, t_time, m_size))\n",
        "\n",
        "# Show final results\n",
        "print(\"\\nFinal Results:\")\n",
        "for r in results:\n",
        "    print(f\"{r[0]} | Seq: {r[1]} | Loss: {r[2]:.4f} | Acc: {r[3]:.4f} | Time: {r[4]:.2f}s | Size: {r[5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZi2Kbq4DE1_",
        "outputId": "4697d484-833d-4d55-d56d-94c682a07393"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing sequence length: 20\n",
            "Training LSTM model with seq_len 20\n",
            "Epoch 10: Train 1.8000, Val 1.8293, Acc 0.4620\n",
            "Epoch 20: Train 1.8491, Val 1.8675, Acc 0.4493\n",
            "Epoch 30: Train 1.8838, Val 1.8950, Acc 0.4436\n",
            "Epoch 40: Train 1.9002, Val 1.9067, Acc 0.4412\n",
            "Epoch 50: Train 1.9165, Val 1.9237, Acc 0.4411\n",
            "Training GRU model with seq_len 20\n",
            "Epoch 10: Train 2.0195, Val 2.0394, Acc 0.4036\n",
            "Epoch 20: Train 2.0560, Val 2.0622, Acc 0.4108\n",
            "Epoch 30: Train 2.0707, Val 2.0946, Acc 0.3990\n",
            "Epoch 40: Train 2.1019, Val 2.1088, Acc 0.3884\n",
            "Epoch 50: Train 2.1130, Val 2.1364, Acc 0.3908\n",
            "\n",
            "Processing sequence length: 30\n",
            "Training LSTM model with seq_len 30\n",
            "Epoch 10: Train 1.7862, Val 1.7984, Acc 0.4667\n",
            "Epoch 20: Train 1.8458, Val 1.8456, Acc 0.4605\n",
            "Epoch 30: Train 1.8799, Val 1.8894, Acc 0.4495\n",
            "Epoch 40: Train 1.8849, Val 1.8975, Acc 0.4483\n",
            "Epoch 50: Train 1.8977, Val 1.8998, Acc 0.4473\n",
            "Training GRU model with seq_len 30\n",
            "Epoch 10: Train 2.0078, Val 2.0031, Acc 0.4127\n",
            "Epoch 20: Train 2.0619, Val 2.0662, Acc 0.3999\n",
            "Epoch 30: Train 2.0928, Val 2.0871, Acc 0.3955\n",
            "Epoch 40: Train 2.1132, Val 2.1076, Acc 0.3861\n",
            "Epoch 50: Train 2.1315, Val 2.1288, Acc 0.3843\n",
            "\n",
            "Final Results:\n",
            "LSTM | Seq: 20 | Loss: 1.9165 | Acc: 0.4411 | Time: 671.10s | Size: 41665\n",
            "GRU | Seq: 20 | Loss: 2.1130 | Acc: 0.3908 | Time: 658.53s | Size: 33345\n",
            "LSTM | Seq: 30 | Loss: 1.8977 | Acc: 0.4473 | Time: 731.83s | Size: 41665\n",
            "GRU | Seq: 30 | Loss: 2.1315 | Acc: 0.3843 | Time: 715.47s | Size: 33345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''What if we increase the sequence length to 50? Perform the training and report the accuracy and model complexity results.'''\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "#  Get the Data\n",
        "def download_shakespeare():\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    r = requests.get(url)\n",
        "    return r.text\n",
        "\n",
        "#   Create Sequences\n",
        "def make_sequences(seq_length, raw_text):\n",
        "    # Create character-to-index map\n",
        "    all_chars = sorted(set(raw_text))\n",
        "    mapping = {ch: i for i, ch in enumerate(all_chars)}\n",
        "\n",
        "    # Encode the text and build sequence lists\n",
        "    encoded = [mapping[ch] for ch in raw_text]\n",
        "    seqs = [encoded[i:i+seq_length] for i in range(len(encoded) - seq_length)]\n",
        "    targets = [encoded[i+seq_length] for i in range(len(encoded) - seq_length)]\n",
        "    return torch.tensor(seqs, dtype=torch.long), torch.tensor(targets, dtype=torch.long), mapping\n",
        "\n",
        "#   Dataset Definition\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.seqs = sequences\n",
        "        self.targs = targets\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seqs[idx], self.targs[idx]\n",
        "\n",
        "#   Define the Model\n",
        "class MyRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, cell_type='LSTM'):\n",
        "        super(MyRNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        # Use either LSTM or GRU based on input\n",
        "        if cell_type.upper() == 'LSTM':\n",
        "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        emb = self.embedding(input_seq)   # [batch, seq_len, hidden_dim]\n",
        "        out, _ = self.rnn(emb)              # [batch, seq_len, hidden_dim]\n",
        "        last_out = out[:, -1, :]            # use the output from the final time step\n",
        "        return self.fc(last_out)\n",
        "\n",
        "#   Training and Evaluation\n",
        "def train_and_test(model, train_loader, valid_loader, num_epochs, lr, dev):\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(dev)\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(dev), labels.to(dev)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()\n",
        "        train_avg = sum_loss / len(train_loader)\n",
        "\n",
        "        # Evaluation step\n",
        "        model.eval()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                inp, lab = batch\n",
        "                inp, lab = inp.to(dev), lab.to(dev)\n",
        "                out = model(inp)\n",
        "                total_loss += loss_function(out, lab).item()\n",
        "                correct += (out.argmax(dim=1) == lab).sum().item()\n",
        "                total += lab.size(0)\n",
        "        valid_avg = total_loss / len(valid_loader)\n",
        "        acc = correct / total\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: TrainLoss = {train_avg:.4f}, ValidLoss = {valid_avg:.4f}, Accuracy = {acc:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    param_total = sum(p.numel() for p in model.parameters())\n",
        "    return train_avg, acc, elapsed, param_total\n",
        "\n",
        "#  Main Experiment (Sequence Length = 50)\n",
        "def main():\n",
        "    seq_length = 50\n",
        "    raw_text = download_shakespeare()\n",
        "    X, Y, char_map = make_sequences(seq_length, raw_text)\n",
        "\n",
        "    dataset = ShakespeareDataset(X, Y)\n",
        "    train_len = int(0.8 * len(dataset))\n",
        "    valid_len = len(dataset) - train_len\n",
        "    train_set, valid_set = random_split(dataset, [train_len, valid_len])\n",
        "    train_dl = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "    valid_dl = DataLoader(valid_set, batch_size=128, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    results = {}\n",
        "    for cell in ['LSTM', 'GRU']:\n",
        "        print(f\"\\nTraining model using {cell} cell with sequence length = {seq_length}\")\n",
        "        net = MyRNNModel(len(char_map), hidden_dim=64, cell_type=cell)\n",
        "        t_loss, v_acc, run_time, num_params = train_and_test(net, train_dl, valid_dl, num_epochs=50, lr=0.01, dev=device)\n",
        "        results[cell] = (t_loss, v_acc, run_time, num_params)\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    for cell_type, metrics in results.items():\n",
        "        tl, acc, rt, params = metrics\n",
        "        print(f\"{cell_type}: TrainLoss = {tl:.4f}, ValidAcc = {acc:.4f}, Time = {rt:.2f}s, Params = {params}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPpYVFJxEHeS",
        "outputId": "e601b80d-dc74-44fb-cc9c-f3b862c026d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model using LSTM cell with sequence length = 50\n",
            "Epoch 10: TrainLoss = 1.7689, ValidLoss = 1.7733, Accuracy = 0.4745\n",
            "Epoch 20: TrainLoss = 1.8365, ValidLoss = 1.8507, Accuracy = 0.4486\n",
            "Epoch 30: TrainLoss = 1.8865, ValidLoss = 1.8929, Accuracy = 0.4483\n",
            "Epoch 40: TrainLoss = 1.8962, ValidLoss = 1.9097, Accuracy = 0.4437\n",
            "Epoch 50: TrainLoss = 1.8935, ValidLoss = 1.8943, Accuracy = 0.4485\n",
            "\n",
            "Training model using GRU cell with sequence length = 50\n",
            "Epoch 10: TrainLoss = 2.0276, ValidLoss = 2.0328, Accuracy = 0.4103\n",
            "Epoch 20: TrainLoss = 2.0654, ValidLoss = 2.0666, Accuracy = 0.3957\n",
            "Epoch 30: TrainLoss = 2.0960, ValidLoss = 2.1114, Accuracy = 0.3945\n",
            "Epoch 40: TrainLoss = 2.1312, ValidLoss = 2.1335, Accuracy = 0.3918\n",
            "Epoch 50: TrainLoss = 2.1695, ValidLoss = 2.1824, Accuracy = 0.3737\n",
            "\n",
            "Final Results:\n",
            "LSTM: TrainLoss = 1.8935, ValidAcc = 0.4485, Time = 727.30s, Params = 41665\n",
            "GRU: TrainLoss = 2.1695, ValidAcc = 0.3737, Time = 673.06s, Params = 33345\n"
          ]
        }
      ]
    }
  ]
}