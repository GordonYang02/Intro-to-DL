{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPL/dfVozmOETwu4okCeUsD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import requests\n",
        "\n",
        "#including runtime measurements, accuracy metrics, and model size calculations (Homework 6)\n",
        "#For training the models with different layers and heads\n",
        "from itertools import product\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "import math\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "#Importing the Swin Transformer model from Hugging Face Transformers library for Problem 3\n",
        "import transformers\n",
        "from transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Check the GPU name and number\n",
        "'''\n",
        "devNumber = torch.cuda.current_device()\n",
        "devName = torch.cuda.get_device_name(devNumber)\n",
        "\n",
        "print(f\"Current device number is: {devNumber}\")\n",
        "print(f\"GPU name is: {devName}\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FhH7l5Ci6xCs",
        "outputId": "4d31a1af-b760-43b6-c545-2c7a3c710931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndevNumber = torch.cuda.current_device()\\ndevName = torch.cuda.get_device_name(devNumber)\\n\\nprint(f\"Current device number is: {devNumber}\")\\nprint(f\"GPU name is: {devName}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1\n",
        "# CIFAR‑100 ─ Vision Transformer scalability study + ResNet‑18 baseline\n",
        "# refactored version (logic preserved, style changed)\n",
        "#\n",
        "\n",
        "# ───────────────────────────── Imports & Globals ──────────────────────────────\n",
        "import time, random, numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary           # <- pip install torchsummary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED          = 42\n",
        "BATCH         = 64\n",
        "EPOCHS_VIT    = 20\n",
        "EPOCHS_RESNET = 10\n",
        "LR            = 0.001\n",
        "NUM_CLASSES   = 100\n",
        "\n",
        "torch.manual_seed(SEED);  np.random.seed(SEED);  random.seed(SEED)\n",
        "\n",
        "# ────────────────────────────── Data pipeline ─────────────────────────────────\n",
        "C100_MEAN = (0.5071, 0.4867, 0.4408)\n",
        "C100_STD  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "transf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(C100_MEAN, C100_STD)\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR100('./data', train=True, download=True,  transform=transf)\n",
        "test_set  = torchvision.datasets.CIFAR100('./data', train=False,               transform=transf)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True,  num_workers=2)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "# ───────────────────────────── ViT building blocks ───────────────────────────\n",
        "class Patchify(nn.Module):\n",
        "    def __init__(self, img=32, patch=4, ch=3, dim=256):\n",
        "        super().__init__()\n",
        "        self.n = (img // patch) ** 2\n",
        "        self.to_patch = nn.Conv2d(ch, dim, patch, patch)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        self.pos = nn.Parameter(torch.zeros(1, self.n + 1, dim))\n",
        "        nn.init.trunc_normal_(self.pos, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.to_patch(x).flatten(2).transpose(1, 2)            # B N D\n",
        "        cls = self.cls_token.expand(B, -1, -1)\n",
        "        return torch.cat([cls, x], 1) + self.pos\n",
        "\n",
        "\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, dim, heads):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "        self.h = heads\n",
        "        self.dk = dim // heads\n",
        "        self.proj_qkv = nn.Linear(dim, dim * 3)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        qkv = self.proj_qkv(x).reshape(B, N, 3, self.h, self.dk).permute(2,0,3,1,4)\n",
        "        q, k, v = qkv\n",
        "        att = (q @ k.transpose(-1,-2)) * (self.dk ** -0.5)\n",
        "        x = (att.softmax(-1) @ v).transpose(1,2).reshape(B, N, D)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ratio=4, p=0.):\n",
        "        super().__init__()\n",
        "        hid = dim * ratio\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hid), nn.GELU(), nn.Dropout(p),\n",
        "            nn.Linear(hid, dim), nn.Dropout(p)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_ratio):\n",
        "        super().__init__()\n",
        "        self.norm1, self.att, self.norm2, self.ffn = (\n",
        "            nn.LayerNorm(dim), MHSA(dim, heads),\n",
        "            nn.LayerNorm(dim), FeedForward(dim, mlp_ratio)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.att(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img=32, patch=4, dim=256, depth=4, heads=4,\n",
        "                 mlp_ratio=4, classes=100):\n",
        "        super().__init__()\n",
        "        self.patch = Patchify(img, patch, 3, dim)\n",
        "        self.body  = nn.Sequential(*[EncoderBlock(dim, heads, mlp_ratio)\n",
        "                                     for _ in range(depth)])\n",
        "        self.norm  = nn.LayerNorm(dim)\n",
        "        self.head  = nn.Linear(dim, classes)\n",
        "        self.apply(self._init)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch(x)\n",
        "        x = self.body(x)\n",
        "        return self.head(self.norm(x[:,0]))\n",
        "\n",
        "# ────────────────────────────── Train / Evaluate ─────────────────────────────\n",
        "def loop(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    hits = tots = 0; t0 = time.time()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        out = model(xb)\n",
        "        loss = crit(out, yb)\n",
        "        if train:\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "        hits += (out.argmax(1) == yb).sum().item()\n",
        "        tots += yb.size(0)\n",
        "    return hits/tots*100, time.time()-t0\n",
        "\n",
        "# ──────────────────────────── Experiment catalogue ───────────────────────────\n",
        "variants = [\n",
        "    dict(tag='ViT‑Tiny',   p=4, d=256, L=4,  H=2, R=2),\n",
        "    dict(tag='ViT‑Small',  p=8, d=256, L=8,  H=2, R=2),\n",
        "    dict(tag='ViT‑Medium', p=4, d=512, L=4,  H=4, R=4),\n",
        "    dict(tag='ViT‑Large',  p=8, d=512, L=8,  H=4, R=4),\n",
        "]\n",
        "\n",
        "log = []\n",
        "for cfg in variants:\n",
        "    print(f\"\\n🟢 Training {cfg['tag']}\")\n",
        "    net = ViT(patch=cfg['p'], dim=cfg['d'], depth=cfg['L'],\n",
        "              heads=cfg['H'], mlp_ratio=cfg['R']).to(device)\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=LR)\n",
        "    summary(net, input_size=(3,32,32), batch_size=BATCH, device=str(device))\n",
        "    epoch_times=[]\n",
        "    for ep in range(1, EPOCHS_VIT+1):\n",
        "        _, sec = loop(net, train_loader, opt)\n",
        "        epoch_times.append(sec)\n",
        "        print(f\"  epoch {ep}/{EPOCHS_VIT} ─ {sec:.2f}s\")\n",
        "    acc,_    = loop(net, test_loader)\n",
        "    params   = sum(p.numel() for p in net.parameters())/1e6\n",
        "    flops_ap = sum(p.numel() for p in net.parameters() if p.requires_grad)*2*32*32/1e9\n",
        "    log.append((cfg['tag'], cfg['p'], cfg['d'], cfg['L'], cfg['H'], cfg['R'],\n",
        "                params, flops_ap, np.mean(epoch_times), acc))\n",
        "\n",
        "# ───────────────────────────── ResNet‑18 baseline ────────────────────────────\n",
        "print(\"\\n🟢 Training ResNet‑18 baseline\")\n",
        "res = torchvision.models.resnet18(num_classes=NUM_CLASSES).to(device)\n",
        "summary(res, input_size=(3,32,32), batch_size=BATCH, device=str(device))\n",
        "opt = torch.optim.Adam(res.parameters(), lr=LR)\n",
        "epoch_times=[]\n",
        "for ep in range(1, EPOCHS_RESNET+1):\n",
        "    _, sec = loop(res, train_loader, opt)\n",
        "    epoch_times.append(sec)\n",
        "    print(f\"  epoch {ep}/{EPOCHS_RESNET} ─ {sec:.2f}s\")\n",
        "acc,_   = loop(res, test_loader)\n",
        "params  = sum(p.numel() for p in res.parameters())/1e6\n",
        "flops   = sum(p.numel() for p in res.parameters() if p.requires_grad)*2*32*32/1e9\n",
        "log.append(('ResNet‑18','N/A','N/A',18,'N/A','N/A',params,flops,np.mean(epoch_times),acc))\n",
        "\n",
        "# ──────────────────────────────── Final table ────────────────────────────────\n",
        "print(\"\\n\" + \"=\"*118)\n",
        "hdr = (\"Model\",\"Patch\",\"Embed\",\"Depth\",\"Heads\",\"MLP\",\n",
        "       \"Params(M)\",\"FLOPs(G)\",\"Time/Epoch(s)\",\"Accuracy\")\n",
        "print(\"{:<15}{:<8}{:<8}{:<8}{:<8}{:<8}{:<15}{:<15}{:<15}{:<10}\".format(*hdr))\n",
        "print(\"-\"*118)\n",
        "for r in log:\n",
        "    print(\"{:<15}{:<8}{:<8}{:<8}{:<8}{:<8}{:<15.2f}{:<15.2f}{:<15.2f}{:<10.2f}\".format(*r))\n",
        "print(\"=\"*118)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZMe23luD-60",
        "outputId": "e5bc2f15-13bf-4169-939e-5bcc1c2e8664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Training ViT‑Tiny\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [64, 256, 8, 8]          12,544\n",
            "          Patchify-2              [64, 65, 256]               0\n",
            "         LayerNorm-3              [64, 65, 256]             512\n",
            "            Linear-4              [64, 65, 768]         197,376\n",
            "            Linear-5              [64, 65, 256]          65,792\n",
            "              MHSA-6              [64, 65, 256]               0\n",
            "         LayerNorm-7              [64, 65, 256]             512\n",
            "            Linear-8              [64, 65, 512]         131,584\n",
            "              GELU-9              [64, 65, 512]               0\n",
            "          Dropout-10              [64, 65, 512]               0\n",
            "           Linear-11              [64, 65, 256]         131,328\n",
            "          Dropout-12              [64, 65, 256]               0\n",
            "      FeedForward-13              [64, 65, 256]               0\n",
            "     EncoderBlock-14              [64, 65, 256]               0\n",
            "        LayerNorm-15              [64, 65, 256]             512\n",
            "           Linear-16              [64, 65, 768]         197,376\n",
            "           Linear-17              [64, 65, 256]          65,792\n",
            "             MHSA-18              [64, 65, 256]               0\n",
            "        LayerNorm-19              [64, 65, 256]             512\n",
            "           Linear-20              [64, 65, 512]         131,584\n",
            "             GELU-21              [64, 65, 512]               0\n",
            "          Dropout-22              [64, 65, 512]               0\n",
            "           Linear-23              [64, 65, 256]         131,328\n",
            "          Dropout-24              [64, 65, 256]               0\n",
            "      FeedForward-25              [64, 65, 256]               0\n",
            "     EncoderBlock-26              [64, 65, 256]               0\n",
            "        LayerNorm-27              [64, 65, 256]             512\n",
            "           Linear-28              [64, 65, 768]         197,376\n",
            "           Linear-29              [64, 65, 256]          65,792\n",
            "             MHSA-30              [64, 65, 256]               0\n",
            "        LayerNorm-31              [64, 65, 256]             512\n",
            "           Linear-32              [64, 65, 512]         131,584\n",
            "             GELU-33              [64, 65, 512]               0\n",
            "          Dropout-34              [64, 65, 512]               0\n",
            "           Linear-35              [64, 65, 256]         131,328\n",
            "          Dropout-36              [64, 65, 256]               0\n",
            "      FeedForward-37              [64, 65, 256]               0\n",
            "     EncoderBlock-38              [64, 65, 256]               0\n",
            "        LayerNorm-39              [64, 65, 256]             512\n",
            "           Linear-40              [64, 65, 768]         197,376\n",
            "           Linear-41              [64, 65, 256]          65,792\n",
            "             MHSA-42              [64, 65, 256]               0\n",
            "        LayerNorm-43              [64, 65, 256]             512\n",
            "           Linear-44              [64, 65, 512]         131,584\n",
            "             GELU-45              [64, 65, 512]               0\n",
            "          Dropout-46              [64, 65, 512]               0\n",
            "           Linear-47              [64, 65, 256]         131,328\n",
            "          Dropout-48              [64, 65, 256]               0\n",
            "      FeedForward-49              [64, 65, 256]               0\n",
            "     EncoderBlock-50              [64, 65, 256]               0\n",
            "        LayerNorm-51                  [64, 256]             512\n",
            "           Linear-52                  [64, 100]          25,700\n",
            "================================================================\n",
            "Total params: 2,147,172\n",
            "Trainable params: 2,147,172\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 568.80\n",
            "Params size (MB): 8.19\n",
            "Estimated Total Size (MB): 577.74\n",
            "----------------------------------------------------------------\n",
            "  epoch 1/20 ─ 10.09s\n",
            "  epoch 2/20 ─ 10.34s\n",
            "  epoch 3/20 ─ 11.40s\n",
            "  epoch 4/20 ─ 10.36s\n",
            "  epoch 5/20 ─ 10.70s\n",
            "  epoch 6/20 ─ 9.97s\n",
            "  epoch 7/20 ─ 10.25s\n",
            "  epoch 8/20 ─ 10.58s\n",
            "  epoch 9/20 ─ 10.47s\n",
            "  epoch 10/20 ─ 10.59s\n",
            "  epoch 11/20 ─ 11.19s\n",
            "  epoch 12/20 ─ 10.09s\n",
            "  epoch 13/20 ─ 10.04s\n",
            "  epoch 14/20 ─ 10.74s\n",
            "  epoch 15/20 ─ 10.22s\n",
            "  epoch 16/20 ─ 10.59s\n",
            "  epoch 17/20 ─ 9.73s\n",
            "  epoch 18/20 ─ 10.59s\n",
            "  epoch 19/20 ─ 10.57s\n",
            "  epoch 20/20 ─ 10.24s\n",
            "\n",
            "🟢 Training ViT‑Small\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [64, 256, 4, 4]          49,408\n",
            "          Patchify-2              [64, 17, 256]               0\n",
            "         LayerNorm-3              [64, 17, 256]             512\n",
            "            Linear-4              [64, 17, 768]         197,376\n",
            "            Linear-5              [64, 17, 256]          65,792\n",
            "              MHSA-6              [64, 17, 256]               0\n",
            "         LayerNorm-7              [64, 17, 256]             512\n",
            "            Linear-8              [64, 17, 512]         131,584\n",
            "              GELU-9              [64, 17, 512]               0\n",
            "          Dropout-10              [64, 17, 512]               0\n",
            "           Linear-11              [64, 17, 256]         131,328\n",
            "          Dropout-12              [64, 17, 256]               0\n",
            "      FeedForward-13              [64, 17, 256]               0\n",
            "     EncoderBlock-14              [64, 17, 256]               0\n",
            "        LayerNorm-15              [64, 17, 256]             512\n",
            "           Linear-16              [64, 17, 768]         197,376\n",
            "           Linear-17              [64, 17, 256]          65,792\n",
            "             MHSA-18              [64, 17, 256]               0\n",
            "        LayerNorm-19              [64, 17, 256]             512\n",
            "           Linear-20              [64, 17, 512]         131,584\n",
            "             GELU-21              [64, 17, 512]               0\n",
            "          Dropout-22              [64, 17, 512]               0\n",
            "           Linear-23              [64, 17, 256]         131,328\n",
            "          Dropout-24              [64, 17, 256]               0\n",
            "      FeedForward-25              [64, 17, 256]               0\n",
            "     EncoderBlock-26              [64, 17, 256]               0\n",
            "        LayerNorm-27              [64, 17, 256]             512\n",
            "           Linear-28              [64, 17, 768]         197,376\n",
            "           Linear-29              [64, 17, 256]          65,792\n",
            "             MHSA-30              [64, 17, 256]               0\n",
            "        LayerNorm-31              [64, 17, 256]             512\n",
            "           Linear-32              [64, 17, 512]         131,584\n",
            "             GELU-33              [64, 17, 512]               0\n",
            "          Dropout-34              [64, 17, 512]               0\n",
            "           Linear-35              [64, 17, 256]         131,328\n",
            "          Dropout-36              [64, 17, 256]               0\n",
            "      FeedForward-37              [64, 17, 256]               0\n",
            "     EncoderBlock-38              [64, 17, 256]               0\n",
            "        LayerNorm-39              [64, 17, 256]             512\n",
            "           Linear-40              [64, 17, 768]         197,376\n",
            "           Linear-41              [64, 17, 256]          65,792\n",
            "             MHSA-42              [64, 17, 256]               0\n",
            "        LayerNorm-43              [64, 17, 256]             512\n",
            "           Linear-44              [64, 17, 512]         131,584\n",
            "             GELU-45              [64, 17, 512]               0\n",
            "          Dropout-46              [64, 17, 512]               0\n",
            "           Linear-47              [64, 17, 256]         131,328\n",
            "          Dropout-48              [64, 17, 256]               0\n",
            "      FeedForward-49              [64, 17, 256]               0\n",
            "     EncoderBlock-50              [64, 17, 256]               0\n",
            "        LayerNorm-51              [64, 17, 256]             512\n",
            "           Linear-52              [64, 17, 768]         197,376\n",
            "           Linear-53              [64, 17, 256]          65,792\n",
            "             MHSA-54              [64, 17, 256]               0\n",
            "        LayerNorm-55              [64, 17, 256]             512\n",
            "           Linear-56              [64, 17, 512]         131,584\n",
            "             GELU-57              [64, 17, 512]               0\n",
            "          Dropout-58              [64, 17, 512]               0\n",
            "           Linear-59              [64, 17, 256]         131,328\n",
            "          Dropout-60              [64, 17, 256]               0\n",
            "      FeedForward-61              [64, 17, 256]               0\n",
            "     EncoderBlock-62              [64, 17, 256]               0\n",
            "        LayerNorm-63              [64, 17, 256]             512\n",
            "           Linear-64              [64, 17, 768]         197,376\n",
            "           Linear-65              [64, 17, 256]          65,792\n",
            "             MHSA-66              [64, 17, 256]               0\n",
            "        LayerNorm-67              [64, 17, 256]             512\n",
            "           Linear-68              [64, 17, 512]         131,584\n",
            "             GELU-69              [64, 17, 512]               0\n",
            "          Dropout-70              [64, 17, 512]               0\n",
            "           Linear-71              [64, 17, 256]         131,328\n",
            "          Dropout-72              [64, 17, 256]               0\n",
            "      FeedForward-73              [64, 17, 256]               0\n",
            "     EncoderBlock-74              [64, 17, 256]               0\n",
            "        LayerNorm-75              [64, 17, 256]             512\n",
            "           Linear-76              [64, 17, 768]         197,376\n",
            "           Linear-77              [64, 17, 256]          65,792\n",
            "             MHSA-78              [64, 17, 256]               0\n",
            "        LayerNorm-79              [64, 17, 256]             512\n",
            "           Linear-80              [64, 17, 512]         131,584\n",
            "             GELU-81              [64, 17, 512]               0\n",
            "          Dropout-82              [64, 17, 512]               0\n",
            "           Linear-83              [64, 17, 256]         131,328\n",
            "          Dropout-84              [64, 17, 256]               0\n",
            "      FeedForward-85              [64, 17, 256]               0\n",
            "     EncoderBlock-86              [64, 17, 256]               0\n",
            "        LayerNorm-87              [64, 17, 256]             512\n",
            "           Linear-88              [64, 17, 768]         197,376\n",
            "           Linear-89              [64, 17, 256]          65,792\n",
            "             MHSA-90              [64, 17, 256]               0\n",
            "        LayerNorm-91              [64, 17, 256]             512\n",
            "           Linear-92              [64, 17, 512]         131,584\n",
            "             GELU-93              [64, 17, 512]               0\n",
            "          Dropout-94              [64, 17, 512]               0\n",
            "           Linear-95              [64, 17, 256]         131,328\n",
            "          Dropout-96              [64, 17, 256]               0\n",
            "      FeedForward-97              [64, 17, 256]               0\n",
            "     EncoderBlock-98              [64, 17, 256]               0\n",
            "        LayerNorm-99                  [64, 256]             512\n",
            "          Linear-100                  [64, 100]          25,700\n",
            "================================================================\n",
            "Total params: 4,292,452\n",
            "Trainable params: 4,292,452\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 293.30\n",
            "Params size (MB): 16.37\n",
            "Estimated Total Size (MB): 310.42\n",
            "----------------------------------------------------------------\n",
            "  epoch 1/20 ─ 17.31s\n",
            "  epoch 2/20 ─ 17.67s\n",
            "  epoch 3/20 ─ 16.63s\n",
            "  epoch 4/20 ─ 16.51s\n",
            "  epoch 5/20 ─ 16.57s\n",
            "  epoch 6/20 ─ 16.68s\n",
            "  epoch 7/20 ─ 16.98s\n",
            "  epoch 8/20 ─ 17.59s\n",
            "  epoch 9/20 ─ 17.73s\n",
            "  epoch 10/20 ─ 16.40s\n",
            "  epoch 11/20 ─ 16.63s\n",
            "  epoch 12/20 ─ 17.30s\n",
            "  epoch 13/20 ─ 17.51s\n",
            "  epoch 14/20 ─ 16.34s\n",
            "  epoch 15/20 ─ 17.41s\n",
            "  epoch 16/20 ─ 16.90s\n",
            "  epoch 17/20 ─ 16.97s\n",
            "  epoch 18/20 ─ 16.56s\n",
            "  epoch 19/20 ─ 16.49s\n",
            "  epoch 20/20 ─ 18.19s\n",
            "\n",
            "🟢 Training ViT‑Medium\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [64, 512, 8, 8]          25,088\n",
            "          Patchify-2              [64, 65, 512]               0\n",
            "         LayerNorm-3              [64, 65, 512]           1,024\n",
            "            Linear-4             [64, 65, 1536]         787,968\n",
            "            Linear-5              [64, 65, 512]         262,656\n",
            "              MHSA-6              [64, 65, 512]               0\n",
            "         LayerNorm-7              [64, 65, 512]           1,024\n",
            "            Linear-8             [64, 65, 2048]       1,050,624\n",
            "              GELU-9             [64, 65, 2048]               0\n",
            "          Dropout-10             [64, 65, 2048]               0\n",
            "           Linear-11              [64, 65, 512]       1,049,088\n",
            "          Dropout-12              [64, 65, 512]               0\n",
            "      FeedForward-13              [64, 65, 512]               0\n",
            "     EncoderBlock-14              [64, 65, 512]               0\n",
            "        LayerNorm-15              [64, 65, 512]           1,024\n",
            "           Linear-16             [64, 65, 1536]         787,968\n",
            "           Linear-17              [64, 65, 512]         262,656\n",
            "             MHSA-18              [64, 65, 512]               0\n",
            "        LayerNorm-19              [64, 65, 512]           1,024\n",
            "           Linear-20             [64, 65, 2048]       1,050,624\n",
            "             GELU-21             [64, 65, 2048]               0\n",
            "          Dropout-22             [64, 65, 2048]               0\n",
            "           Linear-23              [64, 65, 512]       1,049,088\n",
            "          Dropout-24              [64, 65, 512]               0\n",
            "      FeedForward-25              [64, 65, 512]               0\n",
            "     EncoderBlock-26              [64, 65, 512]               0\n",
            "        LayerNorm-27              [64, 65, 512]           1,024\n",
            "           Linear-28             [64, 65, 1536]         787,968\n",
            "           Linear-29              [64, 65, 512]         262,656\n",
            "             MHSA-30              [64, 65, 512]               0\n",
            "        LayerNorm-31              [64, 65, 512]           1,024\n",
            "           Linear-32             [64, 65, 2048]       1,050,624\n",
            "             GELU-33             [64, 65, 2048]               0\n",
            "          Dropout-34             [64, 65, 2048]               0\n",
            "           Linear-35              [64, 65, 512]       1,049,088\n",
            "          Dropout-36              [64, 65, 512]               0\n",
            "      FeedForward-37              [64, 65, 512]               0\n",
            "     EncoderBlock-38              [64, 65, 512]               0\n",
            "        LayerNorm-39              [64, 65, 512]           1,024\n",
            "           Linear-40             [64, 65, 1536]         787,968\n",
            "           Linear-41              [64, 65, 512]         262,656\n",
            "             MHSA-42              [64, 65, 512]               0\n",
            "        LayerNorm-43              [64, 65, 512]           1,024\n",
            "           Linear-44             [64, 65, 2048]       1,050,624\n",
            "             GELU-45             [64, 65, 2048]               0\n",
            "          Dropout-46             [64, 65, 2048]               0\n",
            "           Linear-47              [64, 65, 512]       1,049,088\n",
            "          Dropout-48              [64, 65, 512]               0\n",
            "      FeedForward-49              [64, 65, 512]               0\n",
            "     EncoderBlock-50              [64, 65, 512]               0\n",
            "        LayerNorm-51                  [64, 512]           1,024\n",
            "           Linear-52                  [64, 100]          51,300\n",
            "================================================================\n",
            "Total params: 12,686,948\n",
            "Trainable params: 12,686,948\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 1527.55\n",
            "Params size (MB): 48.40\n",
            "Estimated Total Size (MB): 1576.70\n",
            "----------------------------------------------------------------\n",
            "  epoch 1/20 ─ 20.30s\n",
            "  epoch 2/20 ─ 20.24s\n",
            "  epoch 3/20 ─ 20.27s\n",
            "  epoch 4/20 ─ 20.27s\n",
            "  epoch 5/20 ─ 20.31s\n",
            "  epoch 6/20 ─ 20.31s\n",
            "  epoch 7/20 ─ 20.32s\n",
            "  epoch 8/20 ─ 20.26s\n",
            "  epoch 9/20 ─ 20.31s\n",
            "  epoch 10/20 ─ 20.32s\n",
            "  epoch 11/20 ─ 20.26s\n",
            "  epoch 12/20 ─ 20.29s\n",
            "  epoch 13/20 ─ 20.27s\n",
            "  epoch 14/20 ─ 20.30s\n",
            "  epoch 15/20 ─ 20.31s\n",
            "  epoch 16/20 ─ 20.28s\n",
            "  epoch 17/20 ─ 20.29s\n",
            "  epoch 18/20 ─ 20.32s\n",
            "  epoch 19/20 ─ 20.31s\n",
            "  epoch 20/20 ─ 20.27s\n",
            "\n",
            "🟢 Training ViT‑Large\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [64, 512, 4, 4]          98,816\n",
            "          Patchify-2              [64, 17, 512]               0\n",
            "         LayerNorm-3              [64, 17, 512]           1,024\n",
            "            Linear-4             [64, 17, 1536]         787,968\n",
            "            Linear-5              [64, 17, 512]         262,656\n",
            "              MHSA-6              [64, 17, 512]               0\n",
            "         LayerNorm-7              [64, 17, 512]           1,024\n",
            "            Linear-8             [64, 17, 2048]       1,050,624\n",
            "              GELU-9             [64, 17, 2048]               0\n",
            "          Dropout-10             [64, 17, 2048]               0\n",
            "           Linear-11              [64, 17, 512]       1,049,088\n",
            "          Dropout-12              [64, 17, 512]               0\n",
            "      FeedForward-13              [64, 17, 512]               0\n",
            "     EncoderBlock-14              [64, 17, 512]               0\n",
            "        LayerNorm-15              [64, 17, 512]           1,024\n",
            "           Linear-16             [64, 17, 1536]         787,968\n",
            "           Linear-17              [64, 17, 512]         262,656\n",
            "             MHSA-18              [64, 17, 512]               0\n",
            "        LayerNorm-19              [64, 17, 512]           1,024\n",
            "           Linear-20             [64, 17, 2048]       1,050,624\n",
            "             GELU-21             [64, 17, 2048]               0\n",
            "          Dropout-22             [64, 17, 2048]               0\n",
            "           Linear-23              [64, 17, 512]       1,049,088\n",
            "          Dropout-24              [64, 17, 512]               0\n",
            "      FeedForward-25              [64, 17, 512]               0\n",
            "     EncoderBlock-26              [64, 17, 512]               0\n",
            "        LayerNorm-27              [64, 17, 512]           1,024\n",
            "           Linear-28             [64, 17, 1536]         787,968\n",
            "           Linear-29              [64, 17, 512]         262,656\n",
            "             MHSA-30              [64, 17, 512]               0\n",
            "        LayerNorm-31              [64, 17, 512]           1,024\n",
            "           Linear-32             [64, 17, 2048]       1,050,624\n",
            "             GELU-33             [64, 17, 2048]               0\n",
            "          Dropout-34             [64, 17, 2048]               0\n",
            "           Linear-35              [64, 17, 512]       1,049,088\n",
            "          Dropout-36              [64, 17, 512]               0\n",
            "      FeedForward-37              [64, 17, 512]               0\n",
            "     EncoderBlock-38              [64, 17, 512]               0\n",
            "        LayerNorm-39              [64, 17, 512]           1,024\n",
            "           Linear-40             [64, 17, 1536]         787,968\n",
            "           Linear-41              [64, 17, 512]         262,656\n",
            "             MHSA-42              [64, 17, 512]               0\n",
            "        LayerNorm-43              [64, 17, 512]           1,024\n",
            "           Linear-44             [64, 17, 2048]       1,050,624\n",
            "             GELU-45             [64, 17, 2048]               0\n",
            "          Dropout-46             [64, 17, 2048]               0\n",
            "           Linear-47              [64, 17, 512]       1,049,088\n",
            "          Dropout-48              [64, 17, 512]               0\n",
            "      FeedForward-49              [64, 17, 512]               0\n",
            "     EncoderBlock-50              [64, 17, 512]               0\n",
            "        LayerNorm-51              [64, 17, 512]           1,024\n",
            "           Linear-52             [64, 17, 1536]         787,968\n",
            "           Linear-53              [64, 17, 512]         262,656\n",
            "             MHSA-54              [64, 17, 512]               0\n",
            "        LayerNorm-55              [64, 17, 512]           1,024\n",
            "           Linear-56             [64, 17, 2048]       1,050,624\n",
            "             GELU-57             [64, 17, 2048]               0\n",
            "          Dropout-58             [64, 17, 2048]               0\n",
            "           Linear-59              [64, 17, 512]       1,049,088\n",
            "          Dropout-60              [64, 17, 512]               0\n",
            "      FeedForward-61              [64, 17, 512]               0\n",
            "     EncoderBlock-62              [64, 17, 512]               0\n",
            "        LayerNorm-63              [64, 17, 512]           1,024\n",
            "           Linear-64             [64, 17, 1536]         787,968\n",
            "           Linear-65              [64, 17, 512]         262,656\n",
            "             MHSA-66              [64, 17, 512]               0\n",
            "        LayerNorm-67              [64, 17, 512]           1,024\n",
            "           Linear-68             [64, 17, 2048]       1,050,624\n",
            "             GELU-69             [64, 17, 2048]               0\n",
            "          Dropout-70             [64, 17, 2048]               0\n",
            "           Linear-71              [64, 17, 512]       1,049,088\n",
            "          Dropout-72              [64, 17, 512]               0\n",
            "      FeedForward-73              [64, 17, 512]               0\n",
            "     EncoderBlock-74              [64, 17, 512]               0\n",
            "        LayerNorm-75              [64, 17, 512]           1,024\n",
            "           Linear-76             [64, 17, 1536]         787,968\n",
            "           Linear-77              [64, 17, 512]         262,656\n",
            "             MHSA-78              [64, 17, 512]               0\n",
            "        LayerNorm-79              [64, 17, 512]           1,024\n",
            "           Linear-80             [64, 17, 2048]       1,050,624\n",
            "             GELU-81             [64, 17, 2048]               0\n",
            "          Dropout-82             [64, 17, 2048]               0\n",
            "           Linear-83              [64, 17, 512]       1,049,088\n",
            "          Dropout-84              [64, 17, 512]               0\n",
            "      FeedForward-85              [64, 17, 512]               0\n",
            "     EncoderBlock-86              [64, 17, 512]               0\n",
            "        LayerNorm-87              [64, 17, 512]           1,024\n",
            "           Linear-88             [64, 17, 1536]         787,968\n",
            "           Linear-89              [64, 17, 512]         262,656\n",
            "             MHSA-90              [64, 17, 512]               0\n",
            "        LayerNorm-91              [64, 17, 512]           1,024\n",
            "           Linear-92             [64, 17, 2048]       1,050,624\n",
            "             GELU-93             [64, 17, 2048]               0\n",
            "          Dropout-94             [64, 17, 2048]               0\n",
            "           Linear-95              [64, 17, 512]       1,049,088\n",
            "          Dropout-96              [64, 17, 512]               0\n",
            "      FeedForward-97              [64, 17, 512]               0\n",
            "     EncoderBlock-98              [64, 17, 512]               0\n",
            "        LayerNorm-99                  [64, 512]           1,024\n",
            "          Linear-100                  [64, 100]          51,300\n",
            "================================================================\n",
            "Total params: 25,370,212\n",
            "Trainable params: 25,370,212\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 790.55\n",
            "Params size (MB): 96.78\n",
            "Estimated Total Size (MB): 888.08\n",
            "----------------------------------------------------------------\n",
            "  epoch 1/20 ─ 18.41s\n",
            "  epoch 2/20 ─ 19.56s\n",
            "  epoch 3/20 ─ 17.28s\n",
            "  epoch 4/20 ─ 18.06s\n",
            "  epoch 5/20 ─ 17.22s\n",
            "  epoch 6/20 ─ 16.86s\n",
            "  epoch 7/20 ─ 17.07s\n",
            "  epoch 8/20 ─ 19.05s\n",
            "  epoch 9/20 ─ 17.53s\n",
            "  epoch 10/20 ─ 18.11s\n",
            "  epoch 11/20 ─ 17.34s\n",
            "  epoch 12/20 ─ 17.67s\n",
            "  epoch 13/20 ─ 16.90s\n",
            "  epoch 14/20 ─ 17.57s\n",
            "  epoch 15/20 ─ 17.80s\n",
            "  epoch 16/20 ─ 18.02s\n",
            "  epoch 17/20 ─ 17.46s\n",
            "  epoch 18/20 ─ 18.17s\n",
            "  epoch 19/20 ─ 17.08s\n",
            "  epoch 20/20 ─ 18.35s\n",
            "\n",
            "🟢 Training ResNet‑18 baseline\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [64, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [64, 64, 16, 16]             128\n",
            "              ReLU-3           [64, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [64, 64, 8, 8]               0\n",
            "            Conv2d-5             [64, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [64, 64, 8, 8]             128\n",
            "              ReLU-7             [64, 64, 8, 8]               0\n",
            "            Conv2d-8             [64, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [64, 64, 8, 8]             128\n",
            "             ReLU-10             [64, 64, 8, 8]               0\n",
            "       BasicBlock-11             [64, 64, 8, 8]               0\n",
            "           Conv2d-12             [64, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [64, 64, 8, 8]             128\n",
            "             ReLU-14             [64, 64, 8, 8]               0\n",
            "           Conv2d-15             [64, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [64, 64, 8, 8]             128\n",
            "             ReLU-17             [64, 64, 8, 8]               0\n",
            "       BasicBlock-18             [64, 64, 8, 8]               0\n",
            "           Conv2d-19            [64, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20            [64, 128, 4, 4]             256\n",
            "             ReLU-21            [64, 128, 4, 4]               0\n",
            "           Conv2d-22            [64, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23            [64, 128, 4, 4]             256\n",
            "           Conv2d-24            [64, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25            [64, 128, 4, 4]             256\n",
            "             ReLU-26            [64, 128, 4, 4]               0\n",
            "       BasicBlock-27            [64, 128, 4, 4]               0\n",
            "           Conv2d-28            [64, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29            [64, 128, 4, 4]             256\n",
            "             ReLU-30            [64, 128, 4, 4]               0\n",
            "           Conv2d-31            [64, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32            [64, 128, 4, 4]             256\n",
            "             ReLU-33            [64, 128, 4, 4]               0\n",
            "       BasicBlock-34            [64, 128, 4, 4]               0\n",
            "           Conv2d-35            [64, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36            [64, 256, 2, 2]             512\n",
            "             ReLU-37            [64, 256, 2, 2]               0\n",
            "           Conv2d-38            [64, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39            [64, 256, 2, 2]             512\n",
            "           Conv2d-40            [64, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41            [64, 256, 2, 2]             512\n",
            "             ReLU-42            [64, 256, 2, 2]               0\n",
            "       BasicBlock-43            [64, 256, 2, 2]               0\n",
            "           Conv2d-44            [64, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45            [64, 256, 2, 2]             512\n",
            "             ReLU-46            [64, 256, 2, 2]               0\n",
            "           Conv2d-47            [64, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48            [64, 256, 2, 2]             512\n",
            "             ReLU-49            [64, 256, 2, 2]               0\n",
            "       BasicBlock-50            [64, 256, 2, 2]               0\n",
            "           Conv2d-51            [64, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52            [64, 512, 1, 1]           1,024\n",
            "             ReLU-53            [64, 512, 1, 1]               0\n",
            "           Conv2d-54            [64, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55            [64, 512, 1, 1]           1,024\n",
            "           Conv2d-56            [64, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57            [64, 512, 1, 1]           1,024\n",
            "             ReLU-58            [64, 512, 1, 1]               0\n",
            "       BasicBlock-59            [64, 512, 1, 1]               0\n",
            "           Conv2d-60            [64, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61            [64, 512, 1, 1]           1,024\n",
            "             ReLU-62            [64, 512, 1, 1]               0\n",
            "           Conv2d-63            [64, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64            [64, 512, 1, 1]           1,024\n",
            "             ReLU-65            [64, 512, 1, 1]               0\n",
            "       BasicBlock-66            [64, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67            [64, 512, 1, 1]               0\n",
            "           Linear-68                  [64, 100]          51,300\n",
            "================================================================\n",
            "Total params: 11,227,812\n",
            "Trainable params: 11,227,812\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 82.30\n",
            "Params size (MB): 42.83\n",
            "Estimated Total Size (MB): 125.88\n",
            "----------------------------------------------------------------\n",
            "  epoch 1/10 ─ 10.92s\n",
            "  epoch 2/10 ─ 10.86s\n",
            "  epoch 3/10 ─ 11.41s\n",
            "  epoch 4/10 ─ 10.76s\n",
            "  epoch 5/10 ─ 10.30s\n",
            "  epoch 6/10 ─ 10.31s\n",
            "  epoch 7/10 ─ 11.08s\n",
            "  epoch 8/10 ─ 11.93s\n",
            "  epoch 9/10 ─ 10.34s\n",
            "  epoch 10/10 ─ 10.56s\n",
            "\n",
            "======================================================================================================================\n",
            "Model          Patch   Embed   Depth   Heads   MLP     Params(M)      FLOPs(G)       Time/Epoch(s)  Accuracy  \n",
            "----------------------------------------------------------------------------------------------------------------------\n",
            "ViT‑Tiny       4       256     4       2       2       2.16           4.43           10.44          12.77     \n",
            "ViT‑Small      8       256     8       2       2       4.30           8.80           17.02          8.07      \n",
            "ViT‑Medium     4       512     4       4       4       12.72          26.05          20.29          9.49      \n",
            "ViT‑Large      8       512     8       4       4       25.38          51.98          17.77          6.28      \n",
            "ResNet‑18      N/A     N/A     18      N/A     N/A     11.23          22.99          10.85          45.62     \n",
            "======================================================================================================================\n"
          ]
        }
      ]
    }
  ]
}