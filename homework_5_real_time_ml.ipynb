{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UaMaA8y8kXK7",
        "outputId": "2b2df9f8-1bcc-4933-d6bd-b8db8da83698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndevNumber = torch.cuda.current_device()\\ndevName = torch.cuda.get_device_name(devNumber)\\n\\nprint(f\"Current device number is: {devNumber}\")\\nprint(f\"GPU name is: {devName}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import requests\n",
        "\n",
        "#For training the models with different layers and heads\n",
        "from itertools import product\n",
        "\n",
        "#For Document processing in Problems 3 and 4\n",
        "#pip install python-docx\n",
        "from collections import Counter\n",
        "from docx import Document\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Check the GPU name and number\n",
        "'''\n",
        "devNumber = torch.cuda.current_device()\n",
        "devName = torch.cuda.get_device_name(devNumber)\n",
        "\n",
        "print(f\"Current device number is: {devNumber}\")\n",
        "print(f\"GPU name is: {devName}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTjZhztJkXK8",
        "outputId": "60655d4c-1826-4a42-e818-84b3e4d568f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Transformer with sequence length: 10\n",
            "Epoch 10, Loss: 2.5806, Val Loss: 2.6396, Val Acc: 0.2689\n",
            "Epoch 20, Loss: 2.2084, Val Loss: 2.4492, Val Acc: 0.3067\n",
            "Epoch 30, Loss: 1.9263, Val Loss: 2.3725, Val Acc: 0.3508\n",
            "Epoch 40, Loss: 1.6495, Val Loss: 2.3561, Val Acc: 0.3739\n",
            "Epoch 50, Loss: 1.3817, Val Loss: 2.3911, Val Acc: 0.3908\n",
            "\n",
            "Training Transformer with sequence length: 20\n",
            "Epoch 10, Loss: 2.5817, Val Loss: 2.6580, Val Acc: 0.2468\n",
            "Epoch 20, Loss: 2.2576, Val Loss: 2.4979, Val Acc: 0.2954\n",
            "Epoch 30, Loss: 1.9780, Val Loss: 2.3831, Val Acc: 0.3586\n",
            "Epoch 40, Loss: 1.6860, Val Loss: 2.3186, Val Acc: 0.4135\n",
            "Epoch 50, Loss: 1.3948, Val Loss: 2.3322, Val Acc: 0.4072\n",
            "\n",
            "Training Transformer with sequence length: 30\n",
            "Epoch 10, Loss: 2.6281, Val Loss: 2.6028, Val Acc: 0.2542\n",
            "Epoch 20, Loss: 2.3246, Val Loss: 2.4322, Val Acc: 0.2669\n",
            "Epoch 30, Loss: 2.0576, Val Loss: 2.3558, Val Acc: 0.2945\n",
            "Epoch 40, Loss: 1.7719, Val Loss: 2.3053, Val Acc: 0.3453\n",
            "Epoch 50, Loss: 1.4476, Val Loss: 2.4044, Val Acc: 0.3708\n",
            "\n",
            "Transformer Model Results:\n",
            "Seq Len: 10 | Loss: 1.3817, Val Acc: 0.3908, Time: 4.97s, Model Size: 105902\n",
            "Seq Len: 20 | Loss: 1.3948, Val Acc: 0.4072, Time: 11.75s, Model Size: 105902\n",
            "Seq Len: 30 | Loss: 1.4476, Val Acc: 0.3708, Time: 16.82s, Model Size: 105902\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# Problem 1: Transformer-based Next Character Prediction\n",
        "# ========================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========== Dataset ==========\n",
        "text = \"\"\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text.\n",
        "At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of text during the training phase of the model.\n",
        "One of the most popular approaches to next character prediction involves the use of Recurrent Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character. LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character prediction tasks.\n",
        "Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time.\n",
        "Once trained, the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and enable more natural interactions with AI-based chatbots and virtual assistants.\n",
        "In summary, next character prediction plays a crucial role in enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future of text-based technology.\"\"\"\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "def prepare_data(seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        X.append([char_to_ix[ch] for ch in text[i:i+seq_length]])\n",
        "        y.append(char_to_ix[text[i+seq_length]])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ========== Model ==========\n",
        "class TransformerCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model=64, nhead=2, num_layers=2, dim_feedforward=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_embedding\n",
        "        x = x.permute(1, 0, 2)  # seq_len first\n",
        "        out = self.transformer(x)\n",
        "        return self.fc(out[-1])\n",
        "\n",
        "# ========== Training ==========\n",
        "def train_model(seq_len, epochs=40, lr=0.005):\n",
        "    X, y = prepare_data(seq_len)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    model = TransformerCharModel(vocab_size=len(chars), seq_len=seq_len)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model_size = sum(p.numel() for p in model.parameters())\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X_train)\n",
        "        loss = loss_fn(out, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_out = model(X_val)\n",
        "            val_loss = loss_fn(val_out, y_val)\n",
        "            acc = (val_out.argmax(dim=1) == y_val).float().mean().item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[{seq_len}] Epoch {epoch+1}: Train Loss={loss.item():.4f}, Val Loss={val_loss.item():.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    return loss.item(), val_loss.item(), acc, elapsed, model_size\n",
        "\n",
        "# ========== Run Experiments ==========\n",
        "results = []\n",
        "for length in [10, 20, 30]:\n",
        "    print(f\"\\nTraining for sequence length {length}\")\n",
        "    train_loss, val_loss, val_acc, duration, size = train_model(length)\n",
        "    results.append({\n",
        "        \"Seq Len\": length,\n",
        "        \"Train Loss\": train_loss,\n",
        "        \"Val Loss\": val_loss,\n",
        "        \"Val Acc\": val_acc,\n",
        "        \"Time\": duration,\n",
        "        \"Params\": size\n",
        "    })\n",
        "\n",
        "# ========== Summary ==========\n",
        "print(\"\\n==== Final Results ====\")\n",
        "for res in results:\n",
        "    print(f\"SeqLen {res['Seq Len']:>2} | Train Loss: {res['Train Loss']:.4f} | Val Acc: {res['Val Acc']:.4f} | \"\n",
        "          f\"Time: {res['Time']:.2f}s | Params: {res['Params']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Problem 1: RNN-based approach with cross-attention\n",
        "# The RNN-based approach without cross-attention is already implemented in HW3\n",
        "# ========================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "class Config:\n",
        "    hidden_dim = 128\n",
        "    learning_rate = 0.005\n",
        "    num_epochs = 50\n",
        "    sequence_lengths = [10, 20, 30]\n",
        "    rnn_modes = ['LSTM', 'GRU']\n",
        "\n",
        "# === TEXT SETUP ===\n",
        "raw_text = \"\"\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting the next character in a sequence of text...\"\"\"\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "def create_dataset(seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(raw_text) - seq_len):\n",
        "        X.append([char_to_idx[c] for c in raw_text[i:i + seq_len]])\n",
        "        y.append(char_to_idx[raw_text[i + seq_len]])\n",
        "    return torch.tensor(X), torch.tensor(y)\n",
        "\n",
        "# === CUSTOM ATTENTION LAYER ===\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "        attn_scores = self.attn(encoder_outputs).squeeze(-1)  # (batch, seq_len)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)      # (batch, seq_len)\n",
        "        weighted_sum = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden*2)\n",
        "        return weighted_sum.squeeze(1)\n",
        "\n",
        "# === MODEL ===\n",
        "class RecurrentAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, rnn_type='LSTM'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n",
        "        self.rnn = rnn_cls(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attn = SimpleAttention(hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.embedding(x)                   # (batch, seq_len, hidden)\n",
        "        rnn_out, _ = self.rnn(x_embed)                # (batch, seq_len, hidden*2)\n",
        "        context = self.attn(rnn_out)                  # (batch, hidden*2)\n",
        "        return self.out(context)                      # (batch, vocab)\n",
        "\n",
        "# === TRAINING LOOP ===\n",
        "def train_model(model, train_x, train_y, val_x, val_y, cfg):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
        "    model_size = sum(p.numel() for p in model.parameters())\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_x)\n",
        "        loss = criterion(outputs, train_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_x)\n",
        "            val_loss = criterion(val_outputs, val_y).item()\n",
        "            predictions = val_outputs.argmax(dim=1)\n",
        "            accuracy = (predictions == val_y).float().mean().item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[{model.rnn_type}] Epoch {epoch+1}: Train Loss={loss.item():.4f} | Val Loss={val_loss:.4f} | Acc={accuracy:.4f}\")\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    return loss.item(), val_loss, accuracy, duration, model_size\n",
        "\n",
        "# === MAIN EXPERIMENT LOOP ===\n",
        "results = []\n",
        "cfg = Config()\n",
        "\n",
        "for rnn_type in cfg.rnn_modes:\n",
        "    print(f\"\\n== Training RNN: {rnn_type} ==\")\n",
        "    for seq_len in cfg.sequence_lengths:\n",
        "        print(f\"-- Sequence Length: {seq_len}\")\n",
        "        X, y = create_dataset(seq_len)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        model = RecurrentAttentionModel(vocab_size, cfg.hidden_dim, rnn_type)\n",
        "        train_loss, val_loss, val_acc, time_taken, param_count = train_model(\n",
        "            model, X_train, y_train, X_val, y_val, cfg\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'Model': f'{rnn_type}-Attn',\n",
        "            'Seq Len': seq_len,\n",
        "            'Train Loss': train_loss,\n",
        "            'Val Loss': val_loss,\n",
        "            'Accuracy': val_acc,\n",
        "            'Time': time_taken,\n",
        "            'Params': param_count\n",
        "        })\n",
        "\n",
        "# === REPORT RESULTS ===\n",
        "print(\"\\n=== Results Summary ===\")\n",
        "for r in results:\n",
        "    print(f\"{r['Model']} | Seq={r['Seq Len']} | Train Loss={r['Train Loss']:.4f} | \"\n",
        "          f\"Val Loss={r['Val Loss']:.4f} | Acc={r['Accuracy']:.4f} | \"\n",
        "          f\"Time={r['Time']:.2f}s | Params={r['Params']}\")\n"
      ],
      "metadata": {
        "id": "kEjNfFaVtA3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4e3ba8-f324-4e09-b518-842ce2a8a93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Training RNN: LSTM ==\n",
            "-- Sequence Length: 10\n",
            "[LSTM] Epoch 10: Train Loss=1.9722 | Val Loss=3.5993 | Acc=0.0625\n",
            "[LSTM] Epoch 20: Train Loss=0.3456 | Val Loss=4.6127 | Acc=0.0625\n",
            "[LSTM] Epoch 30: Train Loss=0.0197 | Val Loss=5.6436 | Acc=0.0938\n",
            "[LSTM] Epoch 40: Train Loss=0.0035 | Val Loss=6.2638 | Acc=0.0938\n",
            "[LSTM] Epoch 50: Train Loss=0.0015 | Val Loss=6.6444 | Acc=0.0938\n",
            "-- Sequence Length: 20\n",
            "[LSTM] Epoch 10: Train Loss=2.4225 | Val Loss=3.7580 | Acc=0.0333\n",
            "[LSTM] Epoch 20: Train Loss=1.2597 | Val Loss=4.7150 | Acc=0.1000\n",
            "[LSTM] Epoch 30: Train Loss=0.3285 | Val Loss=6.0369 | Acc=0.1000\n",
            "[LSTM] Epoch 40: Train Loss=0.0447 | Val Loss=7.2752 | Acc=0.1000\n",
            "[LSTM] Epoch 50: Train Loss=0.0094 | Val Loss=7.8635 | Acc=0.1000\n",
            "-- Sequence Length: 30\n",
            "[LSTM] Epoch 10: Train Loss=2.6214 | Val Loss=3.7956 | Acc=0.0357\n",
            "[LSTM] Epoch 20: Train Loss=1.7139 | Val Loss=5.1505 | Acc=0.0000\n",
            "[LSTM] Epoch 30: Train Loss=0.6881 | Val Loss=6.5154 | Acc=0.0000\n",
            "[LSTM] Epoch 40: Train Loss=0.1556 | Val Loss=7.7604 | Acc=0.0000\n",
            "[LSTM] Epoch 50: Train Loss=0.0286 | Val Loss=9.3130 | Acc=0.0000\n",
            "\n",
            "== Training RNN: GRU ==\n",
            "-- Sequence Length: 10\n",
            "[GRU] Epoch 10: Train Loss=1.7908 | Val Loss=3.7101 | Acc=0.0625\n",
            "[GRU] Epoch 20: Train Loss=0.6070 | Val Loss=4.4509 | Acc=0.0938\n",
            "[GRU] Epoch 30: Train Loss=0.0671 | Val Loss=5.3650 | Acc=0.1875\n",
            "[GRU] Epoch 40: Train Loss=0.0098 | Val Loss=5.7490 | Acc=0.2500\n",
            "[GRU] Epoch 50: Train Loss=0.0032 | Val Loss=6.2379 | Acc=0.2500\n",
            "-- Sequence Length: 20\n",
            "[GRU] Epoch 10: Train Loss=2.1098 | Val Loss=4.3145 | Acc=0.0000\n",
            "[GRU] Epoch 20: Train Loss=1.0674 | Val Loss=5.4211 | Acc=0.0000\n",
            "[GRU] Epoch 30: Train Loss=0.3466 | Val Loss=6.9768 | Acc=0.0333\n",
            "[GRU] Epoch 40: Train Loss=0.0660 | Val Loss=7.8599 | Acc=0.0333\n",
            "[GRU] Epoch 50: Train Loss=0.0146 | Val Loss=8.7702 | Acc=0.0333\n",
            "-- Sequence Length: 30\n",
            "[GRU] Epoch 10: Train Loss=2.3950 | Val Loss=4.4144 | Acc=0.0000\n",
            "[GRU] Epoch 20: Train Loss=1.4968 | Val Loss=5.6560 | Acc=0.0000\n",
            "[GRU] Epoch 30: Train Loss=0.7079 | Val Loss=6.9090 | Acc=0.0000\n",
            "[GRU] Epoch 40: Train Loss=0.2031 | Val Loss=7.8389 | Acc=0.0000\n",
            "[GRU] Epoch 50: Train Loss=0.0467 | Val Loss=8.8660 | Acc=0.0000\n",
            "\n",
            "=== Results Summary ===\n",
            "LSTM-Attn | Seq=10 | Train Loss=0.0015 | Val Loss=6.6444 | Acc=0.0938 | Time=0.99s | Params=307996\n",
            "LSTM-Attn | Seq=20 | Train Loss=0.0094 | Val Loss=7.8635 | Acc=0.1000 | Time=1.37s | Params=307996\n",
            "LSTM-Attn | Seq=30 | Train Loss=0.0286 | Val Loss=9.3130 | Acc=0.0000 | Time=1.94s | Params=307996\n",
            "GRU-Attn | Seq=10 | Train Loss=0.0032 | Val Loss=6.2379 | Acc=0.2500 | Time=1.22s | Params=241948\n",
            "GRU-Attn | Seq=20 | Train Loss=0.0146 | Val Loss=8.7702 | Acc=0.0333 | Time=2.21s | Params=241948\n",
            "GRU-Attn | Seq=30 | Train Loss=0.0467 | Val Loss=8.8660 | Acc=0.0000 | Time=3.52s | Params=241948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=======================\n",
        "# Problem 2\n",
        "#=======================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import product\n",
        "\n",
        "class CharacterMapper:\n",
        "    def __init__(self, text):\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
        "\n",
        "class TextSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "class PositionalEmbedder(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class ShakespeareTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = PositionalEmbedder(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.position_embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.classifier(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, learning_rate=0.005):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        try:\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            for batch_X, batch_y in self.train_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(batch_X)\n",
        "                loss = self.criterion(output, batch_y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            return total_loss / len(self.train_loader)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nTraining interrupted by user\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during training: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def evaluate(self):\n",
        "        try:\n",
        "            self.model.eval()\n",
        "            total_loss, correct, total = 0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in self.val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "                    output = self.model(batch_X)\n",
        "                    loss = self.criterion(output, batch_y)\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "                    correct += (predicted == batch_y).sum().item()\n",
        "                    total += batch_y.size(0)\n",
        "            return total_loss / len(self.val_loader), correct / total\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during evaluation: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "def prepare_sequences(text, sequence_length):\n",
        "    mapper = CharacterMapper(text)\n",
        "    encoded_text = mapper.encode(text)\n",
        "\n",
        "    sequences, targets = [], []\n",
        "    for i in range(len(encoded_text) - sequence_length):\n",
        "        seq = encoded_text[i:i + sequence_length]\n",
        "        target = encoded_text[i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "\n",
        "    return torch.tensor(sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long), mapper\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load Shakespeare dataset\n",
        "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "        text = requests.get(url).text\n",
        "\n",
        "        # Model configurations\n",
        "        configs = {\n",
        "            \"sequence_lengths\": [20, 30],\n",
        "            \"d_model\": 64,\n",
        "            \"num_layers\": [1, 2, 4],\n",
        "            \"nhead\": [2, 4],\n",
        "            \"epochs\": 10,\n",
        "            \"batch_size\": 64,\n",
        "            \"learning_rate\": 0.005\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "        for num_layers, nhead in product(configs[\"num_layers\"], configs[\"nhead\"]):\n",
        "            print(f\"\\nTransformer: Layers={num_layers} Heads={nhead}\")\n",
        "\n",
        "            for seq_length in configs[\"sequence_lengths\"]:\n",
        "                print(f\"\\nTraining with sequence length: {seq_length}\")\n",
        "\n",
        "                try:\n",
        "                    # Prepare data\n",
        "                    X, y, mapper = prepare_sequences(text, seq_length)\n",
        "                    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                    train_dataset = TextSequenceDataset(X_train, y_train)\n",
        "                    val_dataset = TextSequenceDataset(X_val, y_val)\n",
        "\n",
        "                    train_loader = DataLoader(train_dataset, batch_size=configs[\"batch_size\"], shuffle=True, num_workers=0)\n",
        "                    val_loader = DataLoader(val_dataset, batch_size=configs[\"batch_size\"], shuffle=False, num_workers=0)\n",
        "\n",
        "                    # Initialize model and trainer\n",
        "                    model = ShakespeareTransformer(mapper.vocab_size, configs[\"d_model\"], nhead, num_layers)\n",
        "                    trainer = ModelTrainer(model, train_loader, val_loader, learning_rate=configs[\"learning_rate\"])\n",
        "\n",
        "                    # Training loop\n",
        "                    start_time = time.time()\n",
        "                    for epoch in range(1, configs[\"epochs\"] + 1):\n",
        "                        train_loss = trainer.train_epoch()\n",
        "                        if train_loss is None:\n",
        "                            break\n",
        "\n",
        "                        val_loss, val_acc = trainer.evaluate()\n",
        "                        if val_loss is None or val_acc is None:\n",
        "                            break\n",
        "\n",
        "                        if epoch % 2 == 0:\n",
        "                            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, '\n",
        "                                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "                    exec_time = time.time() - start_time\n",
        "                    model_size = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "                    results.append({\n",
        "                        \"Seq Len\": seq_length,\n",
        "                        \"Layers\": num_layers,\n",
        "                        \"Heads\": nhead,\n",
        "                        \"Loss\": train_loss,\n",
        "                        \"Val Acc\": val_acc,\n",
        "                        \"Time\": exec_time,\n",
        "                        \"Model Size\": model_size\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during training with sequence length {seq_length}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nTransformer Results:\")\n",
        "        for r in results:\n",
        "            print(f\"Seq Len: {r['Seq Len']} | Layers: {r['Layers']} | Heads: {r['Heads']} | \"\n",
        "                  f\"Loss: {r['Loss']:.4f} | Val Acc: {r['Val Acc']:.4f} | Time: {r['Time']:.2f}s | \"\n",
        "                  f\"Model Size: {r['Model Size']}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProgram interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in main program: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIp0dNPczOg0",
        "outputId": "6088da34-d8d7-446b-cf5b-3e3dca923343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformer: Layers=1 Heads=2\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 2.1809, Val Loss: 2.0653, Val Acc: 0.3887\n",
            "Epoch 4, Train Loss: 2.1476, Val Loss: 2.0461, Val Acc: 0.3909\n",
            "Epoch 6, Train Loss: 2.1920, Val Loss: 2.0999, Val Acc: 0.3690\n",
            "Epoch 8, Train Loss: 2.1590, Val Loss: 2.1057, Val Acc: 0.3670\n",
            "Epoch 10, Train Loss: 2.1932, Val Loss: 2.1535, Val Acc: 0.3543\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 2.2237, Val Loss: 2.2230, Val Acc: 0.3338\n",
            "Epoch 4, Train Loss: 2.2700, Val Loss: 2.1819, Val Acc: 0.3460\n",
            "Epoch 6, Train Loss: 2.2300, Val Loss: 2.2253, Val Acc: 0.3305\n",
            "Epoch 8, Train Loss: 2.2326, Val Loss: 2.2138, Val Acc: 0.3401\n",
            "Epoch 10, Train Loss: 2.2197, Val Loss: 2.1971, Val Acc: 0.3447\n",
            "\n",
            "Transformer: Layers=1 Heads=4\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 2.1112, Val Loss: 2.0279, Val Acc: 0.3969\n",
            "Epoch 4, Train Loss: 2.0786, Val Loss: 1.9760, Val Acc: 0.4108\n",
            "Epoch 6, Train Loss: 2.0755, Val Loss: 2.0391, Val Acc: 0.3918\n",
            "Epoch 8, Train Loss: 2.0950, Val Loss: 1.9899, Val Acc: 0.4060\n",
            "Epoch 10, Train Loss: 2.1231, Val Loss: 2.0565, Val Acc: 0.3870\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 2.1255, Val Loss: 2.0466, Val Acc: 0.3918\n",
            "Epoch 4, Train Loss: 2.1098, Val Loss: 2.0155, Val Acc: 0.4015\n",
            "Epoch 6, Train Loss: 2.1085, Val Loss: 2.0199, Val Acc: 0.3972\n",
            "Epoch 8, Train Loss: 2.1060, Val Loss: 2.0575, Val Acc: 0.3847\n",
            "Epoch 10, Train Loss: 2.1747, Val Loss: 2.1132, Val Acc: 0.3688\n",
            "\n",
            "Transformer: Layers=2 Heads=2\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 2.2049, Val Loss: 2.1054, Val Acc: 0.3757\n",
            "Epoch 4, Train Loss: 2.1664, Val Loss: 2.0731, Val Acc: 0.3880\n",
            "Epoch 6, Train Loss: 2.1705, Val Loss: 2.0837, Val Acc: 0.3899\n",
            "Epoch 8, Train Loss: 2.1729, Val Loss: 2.0184, Val Acc: 0.4029\n",
            "Epoch 10, Train Loss: 2.2291, Val Loss: 2.0423, Val Acc: 0.3950\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 2.2009, Val Loss: 2.1085, Val Acc: 0.3753\n",
            "Epoch 4, Train Loss: 2.1963, Val Loss: 2.1085, Val Acc: 0.3763\n",
            "Epoch 6, Train Loss: 2.2259, Val Loss: 2.1262, Val Acc: 0.3761\n",
            "Epoch 8, Train Loss: 2.2723, Val Loss: 2.3391, Val Acc: 0.3248\n",
            "Epoch 10, Train Loss: 2.3065, Val Loss: 2.1689, Val Acc: 0.3581\n",
            "\n",
            "Transformer: Layers=2 Heads=4\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 2.1693, Val Loss: 2.0828, Val Acc: 0.3834\n",
            "Epoch 4, Train Loss: 2.1446, Val Loss: 2.0486, Val Acc: 0.3906\n",
            "Epoch 6, Train Loss: 2.1677, Val Loss: 2.0308, Val Acc: 0.3999\n",
            "Epoch 8, Train Loss: 2.1947, Val Loss: 2.1938, Val Acc: 0.3584\n",
            "Epoch 10, Train Loss: 2.2676, Val Loss: 2.1210, Val Acc: 0.3739\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 2.1611, Val Loss: 2.0599, Val Acc: 0.3898\n",
            "Epoch 4, Train Loss: 2.1685, Val Loss: 2.0892, Val Acc: 0.3847\n",
            "Epoch 6, Train Loss: 2.1810, Val Loss: 2.0985, Val Acc: 0.3788\n",
            "Epoch 8, Train Loss: 2.2086, Val Loss: 2.0810, Val Acc: 0.3799\n",
            "Epoch 10, Train Loss: 2.2405, Val Loss: 2.2031, Val Acc: 0.3562\n",
            "\n",
            "Transformer: Layers=4 Heads=2\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 3.3169, Val Loss: 3.3146, Val Acc: 0.1525\n",
            "Epoch 4, Train Loss: 3.3157, Val Loss: 3.3134, Val Acc: 0.1525\n",
            "Epoch 6, Train Loss: 3.3156, Val Loss: 3.3136, Val Acc: 0.1525\n",
            "Epoch 8, Train Loss: 3.3156, Val Loss: 3.3145, Val Acc: 0.1525\n",
            "Epoch 10, Train Loss: 3.3156, Val Loss: 3.3141, Val Acc: 0.1525\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 3.3172, Val Loss: 3.3137, Val Acc: 0.1530\n",
            "Epoch 4, Train Loss: 3.3160, Val Loss: 3.3120, Val Acc: 0.1530\n",
            "Epoch 6, Train Loss: 3.3158, Val Loss: 3.3144, Val Acc: 0.1530\n",
            "Epoch 8, Train Loss: 3.3158, Val Loss: 3.3128, Val Acc: 0.1530\n",
            "Epoch 10, Train Loss: 3.3158, Val Loss: 3.3125, Val Acc: 0.1530\n",
            "\n",
            "Transformer: Layers=4 Heads=4\n",
            "\n",
            "Training with sequence length: 20\n",
            "Epoch 2, Train Loss: 3.3168, Val Loss: 3.3162, Val Acc: 0.1525\n",
            "Epoch 4, Train Loss: 3.3160, Val Loss: 3.3159, Val Acc: 0.1525\n",
            "Epoch 6, Train Loss: 3.3158, Val Loss: 3.3138, Val Acc: 0.1525\n",
            "Epoch 8, Train Loss: 3.3158, Val Loss: 3.3135, Val Acc: 0.1525\n",
            "Epoch 10, Train Loss: 3.3158, Val Loss: 3.3152, Val Acc: 0.1525\n",
            "\n",
            "Training with sequence length: 30\n",
            "Epoch 2, Train Loss: 3.3171, Val Loss: 3.3129, Val Acc: 0.1530\n",
            "Epoch 4, Train Loss: 3.3162, Val Loss: 3.3121, Val Acc: 0.1530\n",
            "Epoch 6, Train Loss: 3.3161, Val Loss: 3.3120, Val Acc: 0.1530\n",
            "Epoch 8, Train Loss: 3.3160, Val Loss: 3.3134, Val Acc: 0.1530\n",
            "Epoch 10, Train Loss: 3.3161, Val Loss: 3.3125, Val Acc: 0.1530\n",
            "\n",
            "Transformer Results:\n",
            "Seq Len: 20 | Layers: 1 | Heads: 2 | Loss: 2.1932 | Val Acc: 0.3543 | Time: 597.79s | Model Size: 58369\n",
            "Seq Len: 30 | Layers: 1 | Heads: 2 | Loss: 2.2197 | Val Acc: 0.3447 | Time: 605.04s | Model Size: 58369\n",
            "Seq Len: 20 | Layers: 1 | Heads: 4 | Loss: 2.1231 | Val Acc: 0.3870 | Time: 601.57s | Model Size: 58369\n",
            "Seq Len: 30 | Layers: 1 | Heads: 4 | Loss: 2.1747 | Val Acc: 0.3688 | Time: 603.85s | Model Size: 58369\n",
            "Seq Len: 20 | Layers: 2 | Heads: 2 | Loss: 2.2291 | Val Acc: 0.3950 | Time: 909.65s | Model Size: 108353\n",
            "Seq Len: 30 | Layers: 2 | Heads: 2 | Loss: 2.3065 | Val Acc: 0.3581 | Time: 918.09s | Model Size: 108353\n",
            "Seq Len: 20 | Layers: 2 | Heads: 4 | Loss: 2.2676 | Val Acc: 0.3739 | Time: 925.77s | Model Size: 108353\n",
            "Seq Len: 30 | Layers: 2 | Heads: 4 | Loss: 2.2405 | Val Acc: 0.3562 | Time: 926.92s | Model Size: 108353\n",
            "Seq Len: 20 | Layers: 4 | Heads: 2 | Loss: 3.3156 | Val Acc: 0.1525 | Time: 1545.78s | Model Size: 208321\n",
            "Seq Len: 30 | Layers: 4 | Heads: 2 | Loss: 3.3158 | Val Acc: 0.1530 | Time: 1553.71s | Model Size: 208321\n",
            "Seq Len: 20 | Layers: 4 | Heads: 4 | Loss: 3.3158 | Val Acc: 0.1525 | Time: 1545.35s | Model Size: 208321\n",
            "Seq Len: 30 | Layers: 4 | Heads: 4 | Loss: 3.3161 | Val Acc: 0.1530 | Time: 1556.79s | Model Size: 208321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===============\n",
        "# Problem 2 sequence length to 50. Perform the training and report the accuracy and model complexity results.\n",
        "#=================\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load Shakespeare dataset\n",
        "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "        text = requests.get(url).text\n",
        "\n",
        "        # Model configurations\n",
        "        configs = {\n",
        "            \"sequence_lengths\": [50],  # Changed to only use sequence length 50\n",
        "            \"d_model\": 64,\n",
        "            \"num_layers\": [2],  # Fixed to 2 layers\n",
        "            \"nhead\": [4],  # Fixed to 4 heads\n",
        "            \"epochs\": 10,\n",
        "            \"batch_size\": 64,\n",
        "            \"learning_rate\": 0.005\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "        for num_layers, nhead in product(configs[\"num_layers\"], configs[\"nhead\"]):\n",
        "            print(f\"\\nTransformer: Layers={num_layers} Heads={nhead}\")\n",
        "\n",
        "            for seq_length in configs[\"sequence_lengths\"]:\n",
        "                print(f\"\\nTraining with sequence length: {seq_length}\")\n",
        "\n",
        "                try:\n",
        "                    # Prepare data\n",
        "                    X, y, mapper = prepare_sequences(text, seq_length)\n",
        "                    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                    train_dataset = TextSequenceDataset(X_train, y_train)\n",
        "                    val_dataset = TextSequenceDataset(X_val, y_val)\n",
        "\n",
        "                    train_loader = DataLoader(train_dataset, batch_size=configs[\"batch_size\"], shuffle=True, num_workers=0)\n",
        "                    val_loader = DataLoader(val_dataset, batch_size=configs[\"batch_size\"], shuffle=False, num_workers=0)\n",
        "\n",
        "                    # Initialize model and trainer\n",
        "                    model = ShakespeareTransformer(mapper.vocab_size, configs[\"d_model\"], nhead, num_layers)\n",
        "                    trainer = ModelTrainer(model, train_loader, val_loader, learning_rate=configs[\"learning_rate\"])\n",
        "\n",
        "                    # Training loop\n",
        "                    start_time = time.time()\n",
        "                    for epoch in range(1, configs[\"epochs\"] + 1):\n",
        "                        train_loss = trainer.train_epoch()\n",
        "                        if train_loss is None:\n",
        "                            break\n",
        "\n",
        "                        val_loss, val_acc = trainer.evaluate()\n",
        "                        if val_loss is None or val_acc is None:\n",
        "                            break\n",
        "\n",
        "                        if epoch % 2 == 0:\n",
        "                            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, '\n",
        "                                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "                    exec_time = time.time() - start_time\n",
        "                    model_size = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "                    results.append({\n",
        "                        \"Seq Len\": seq_length,\n",
        "                        \"Layers\": num_layers,\n",
        "                        \"Heads\": nhead,\n",
        "                        \"Loss\": train_loss,\n",
        "                        \"Val Acc\": val_acc,\n",
        "                        \"Time\": exec_time,\n",
        "                        \"Model Size\": model_size\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during training with sequence length {seq_length}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nTransformer Results:\")\n",
        "        for r in results:\n",
        "            print(f\"Seq Len: {r['Seq Len']} | Layers: {r['Layers']} | Heads: {r['Heads']} | \"\n",
        "                  f\"Loss: {r['Loss']:.4f} | Val Acc: {r['Val Acc']:.4f} | Time: {r['Time']:.2f}s | \"\n",
        "                  f\"Model Size: {r['Model Size']}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProgram interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in main program: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvM03QTF_Rp4",
        "outputId": "79a2acb4-6d48-4b24-c28c-3bf5b53fe05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformer: Layers=2 Heads=4\n",
            "\n",
            "Training with sequence length: 50\n",
            "Error during training with sequence length 50: PositionalEncoder.__init__() missing 1 required positional argument: 'max_seq_length'\n",
            "\n",
            "Transformer Results:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================\n",
        "# Problem 3\n",
        "#============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from docx import Document\n",
        "import time\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.word_to_idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.idx_to_word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.vocab_size = 4\n",
        "\n",
        "    def process_text(self, text):\n",
        "        words = text.split()\n",
        "        indices = []\n",
        "        for word in words:\n",
        "            if word not in self.word_to_idx:\n",
        "                self.word_to_idx[word] = self.vocab_size\n",
        "                self.idx_to_word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "            indices.append(self.word_to_idx[word])\n",
        "        indices.append(self.word_to_idx[\"<EOS>\"])\n",
        "        return indices\n",
        "\n",
        "    def decode_indices(self, indices):\n",
        "        return \" \".join([self.idx_to_word.get(idx, \"<UNK>\") for idx in indices])\n",
        "\n",
        "class TranslationData:\n",
        "    def __init__(self, docx_path):\n",
        "        self.pairs = self._load_pairs(docx_path)\n",
        "        self.english_processor = TextProcessor()\n",
        "        self.french_processor = TextProcessor()\n",
        "        self._build_vocabularies()\n",
        "\n",
        "    def _load_pairs(self, docx_path):\n",
        "        doc = Document(docx_path)\n",
        "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "        pairs = []\n",
        "        for line in text.split(\"\\n\"):\n",
        "            if '\", \"' in line:\n",
        "                en, fr = line.split('\", \"')\n",
        "                en = en.replace('(\"', '').strip()\n",
        "                fr = fr.replace('\")', '').strip()\n",
        "                pairs.append((en, fr))\n",
        "        return pairs\n",
        "\n",
        "    def _build_vocabularies(self):\n",
        "        for en, fr in self.pairs:\n",
        "            self.english_processor.process_text(en)\n",
        "            self.french_processor.process_text(fr)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, translation_data):\n",
        "        self.translation_data = translation_data\n",
        "        self.pairs = translation_data.pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en, fr = self.pairs[idx]\n",
        "        en_indices = self.translation_data.english_processor.process_text(en)\n",
        "        fr_indices = self.translation_data.french_processor.process_text(fr)\n",
        "        return torch.tensor(en_indices), torch.tensor(fr_indices)\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=100):\n",
        "        super().__init__()\n",
        "        self.encoding = nn.Parameter(torch.zeros(max_seq_length, d_model))\n",
        "        nn.init.normal_(self.encoding, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:x.size(1), :]\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_layers):\n",
        "        super().__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.position_encoder = PositionalEncoder(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_embedded = self.position_encoder(self.src_embedding(src))\n",
        "        tgt_embedded = self.position_encoder(self.tgt_embedding(tgt))\n",
        "\n",
        "        src_embedded = src_embedded.transpose(0, 1)\n",
        "        tgt_embedded = tgt_embedded.transpose(0, 1)\n",
        "\n",
        "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src_embedded,\n",
        "            tgt_embedded,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "\n",
        "        return self.fc_out(output)\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, learning_rate=0.001):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for src, tgt in self.train_loader:\n",
        "            src, tgt = src.to(self.device), tgt.to(self.device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(src, tgt_input)\n",
        "            output = output.permute(1, 2, 0)\n",
        "\n",
        "            loss = self.criterion(output, tgt_output)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in self.train_loader:\n",
        "                src, tgt = src.to(self.device), tgt.to(self.device)\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_output = tgt[:, 1:]\n",
        "\n",
        "                output = self.model(src, tgt_input)\n",
        "                output = output.permute(1, 2, 0)\n",
        "\n",
        "                loss = self.criterion(output, tgt_output)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                preds = output.argmax(dim=1)\n",
        "                mask = tgt_output != 0\n",
        "                correct += (preds[mask] == tgt_output[mask]).sum().item()\n",
        "                total += mask.sum().item()\n",
        "\n",
        "        return total_loss / len(self.train_loader), correct / total if total > 0 else 0\n",
        "\n",
        "class TranslationGenerator:\n",
        "    def __init__(self, model, english_processor, french_processor):\n",
        "        self.model = model\n",
        "        self.english_processor = english_processor\n",
        "        self.french_processor = french_processor\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def generate(self, english_text, max_length=100):\n",
        "        src_indices = self.english_processor.process_text(english_text)\n",
        "        src_tensor = torch.tensor(src_indices).unsqueeze(0).to(self.device)\n",
        "\n",
        "        tgt_indices = [self.french_processor.word_to_idx[\"<SOS>\"]]\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            src_embedded = self.model.position_encoder(self.model.src_embedding(src_tensor))\n",
        "            src_embedded = src_embedded.transpose(0, 1)\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(self.device)\n",
        "                tgt_embedded = self.model.position_encoder(self.model.tgt_embedding(tgt_tensor))\n",
        "                tgt_embedded = tgt_embedded.transpose(0, 1)\n",
        "\n",
        "                output = self.model.transformer(\n",
        "                    src_embedded,\n",
        "                    tgt_embedded,\n",
        "                    src_mask=self.model.transformer.generate_square_subsequent_mask(src_tensor.size(1)).to(self.device),\n",
        "                    tgt_mask=self.model.transformer.generate_square_subsequent_mask(tgt_tensor.size(1)).to(self.device)\n",
        "                )\n",
        "\n",
        "                output = self.model.fc_out(output[-1, :, :])\n",
        "                next_token = output.argmax(dim=1).item()\n",
        "                tgt_indices.append(next_token)\n",
        "\n",
        "                if next_token == self.french_processor.word_to_idx[\"<EOS>\"]:\n",
        "                    break\n",
        "\n",
        "        return self.french_processor.decode_indices(tgt_indices[1:-1])\n",
        "\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    translation_data = TranslationData(\"Dataset - English to French.docx\")\n",
        "    dataset = TranslationDataset(translation_data)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: (\n",
        "        nn.utils.rnn.pad_sequence([item[0] for item in x], batch_first=True, padding_value=0),\n",
        "        nn.utils.rnn.pad_sequence([item[1] for item in x], batch_first=True, padding_value=0)\n",
        "    ))\n",
        "\n",
        "    # Model configurations\n",
        "    configs = {\n",
        "        \"d_model\": 128,\n",
        "        \"num_layers\": [1, 2, 4],\n",
        "        \"nhead\": [2, 4],\n",
        "        \"epochs\": 50,\n",
        "        \"learning_rate\": 0.001\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for num_layers, nhead in product(configs[\"num_layers\"], configs[\"nhead\"]):\n",
        "        print(f\"\\nTransformer: Layers = {num_layers} Heads = {nhead}\")\n",
        "\n",
        "        model = TranslationTransformer(\n",
        "            translation_data.english_processor.vocab_size,\n",
        "            translation_data.french_processor.vocab_size,\n",
        "            configs[\"d_model\"],\n",
        "            nhead,\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        trainer = ModelTrainer(model, train_loader, configs[\"learning_rate\"])\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(1, configs[\"epochs\"] + 1):\n",
        "            train_loss = trainer.train_epoch()\n",
        "            if epoch % 10 == 0 or epoch == configs[\"epochs\"]:\n",
        "                print(f\"Epoch {epoch}/{configs['epochs']} - Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        exec_time = time.time() - start_time\n",
        "        val_loss, val_acc = trainer.evaluate()\n",
        "        model_size = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        # Qualitative validation\n",
        "        generator = TranslationGenerator(model, translation_data.english_processor, translation_data.french_processor)\n",
        "        idx = np.random.randint(len(dataset))\n",
        "        en, fr = dataset.pairs[idx]\n",
        "        translated = generator.generate(en)\n",
        "\n",
        "        print(\"\\nQualitative Validation Example:\")\n",
        "        print(f\"English: {en}\")\n",
        "        print(f\"True French: {fr}\")\n",
        "        print(f\"Predicted French: {translated}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        results.append({\n",
        "            \"Layers\": num_layers,\n",
        "            \"Heads\": nhead,\n",
        "            \"Train Loss\": train_loss,\n",
        "            \"Val Loss\": val_loss,\n",
        "            \"Val Acc\": val_acc,\n",
        "            \"Time\": exec_time,\n",
        "            \"Model Size\": model_size\n",
        "        })\n",
        "\n",
        "    print(\"\\nTransformer Results:\")\n",
        "    for r in results:\n",
        "        print(f\"Layers: {r['Layers']} | Heads: {r['Heads']} | \"\n",
        "              f\"Train Loss: {r['Train Loss']:.4f} | Val Loss: {r['Val Loss']:.4f} | \"\n",
        "              f\"Val Acc: {r['Val Acc']:.4f} | Time: {r['Time']:.2f}s | Model Size: {r['Model Size']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw54QgwHAZkr",
        "outputId": "820e8249-7a6a-4f25-f4ad-38d155bebe3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformer: Layers = 1 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 2.0421\n",
            "Epoch 20/50 - Train Loss: 0.5201\n",
            "Epoch 30/50 - Train Loss: 0.1592\n",
            "Epoch 40/50 - Train Loss: 0.0868\n",
            "Epoch 50/50 - Train Loss: 0.0633\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: He fixes his bicycle\n",
            "True French: Il répare son vélo,\n",
            "Predicted French: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 1 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 2.0800\n",
            "Epoch 20/50 - Train Loss: 0.5069\n",
            "Epoch 30/50 - Train Loss: 0.1477\n",
            "Epoch 40/50 - Train Loss: 0.0742\n",
            "Epoch 50/50 - Train Loss: 0.0449\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: She studies hard for exams\n",
            "True French: Elle étudie dur pour les examens,\n",
            "Predicted French: le bus,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 2 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 2.3126\n",
            "Epoch 20/50 - Train Loss: 0.6363\n",
            "Epoch 30/50 - Train Loss: 0.1661\n",
            "Epoch 40/50 - Train Loss: 0.0687\n",
            "Epoch 50/50 - Train Loss: 0.0471\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: We work in the office\n",
            "True French: Nous travaillons au bureau,\n",
            "Predicted French: la travaillons au bureau,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 2 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 2.0949\n",
            "Epoch 20/50 - Train Loss: 0.6020\n",
            "Epoch 30/50 - Train Loss: 0.1620\n",
            "Epoch 40/50 - Train Loss: 0.0658\n",
            "Epoch 50/50 - Train Loss: 0.0399\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: The birds chirp in the morning\n",
            "True French: Les oiseaux gazouillent le matin,\n",
            "Predicted French: le matin,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 4 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 4.2723\n",
            "Epoch 20/50 - Train Loss: 1.8878\n",
            "Epoch 30/50 - Train Loss: 0.8567\n",
            "Epoch 40/50 - Train Loss: 0.4036\n",
            "Epoch 50/50 - Train Loss: 0.1665\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: The sun sets in the evening\n",
            "True French: Le soleil se couche le soir,\n",
            "Predicted French: le petit déjeuner ensemble,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 4 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 3.7422\n",
            "Epoch 20/50 - Train Loss: 1.5699\n",
            "Epoch 30/50 - Train Loss: 0.6874\n",
            "Epoch 40/50 - Train Loss: 0.3142\n",
            "Epoch 50/50 - Train Loss: 0.1106\n",
            "\n",
            "Qualitative Validation Example:\n",
            "English: He climbs the mountain\n",
            "True French: Il gravit la montagne,\n",
            "Predicted French: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer Results:\n",
            "Layers: 1 | Heads: 2 | Train Loss: 0.0633 | Val Loss: 0.0173 | Val Acc: 1.0000 | Time: 1.92s | Model Size: 1371551\n",
            "Layers: 1 | Heads: 4 | Train Loss: 0.0449 | Val Loss: 0.0153 | Val Acc: 1.0000 | Time: 1.96s | Model Size: 1371551\n",
            "Layers: 2 | Heads: 2 | Train Loss: 0.0471 | Val Loss: 0.0155 | Val Acc: 1.0000 | Time: 3.10s | Model Size: 2623903\n",
            "Layers: 2 | Heads: 4 | Train Loss: 0.0399 | Val Loss: 0.0149 | Val Acc: 1.0000 | Time: 3.18s | Model Size: 2623903\n",
            "Layers: 4 | Heads: 2 | Train Loss: 0.1665 | Val Loss: 0.0762 | Val Acc: 1.0000 | Time: 5.43s | Model Size: 5128607\n",
            "Layers: 4 | Heads: 4 | Train Loss: 0.1106 | Val Loss: 0.0451 | Val Acc: 1.0000 | Time: 5.47s | Model Size: 5128607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================\n",
        "#Problem 4\n",
        "#===========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from docx import Document\n",
        "import time\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "class LanguageProcessor:\n",
        "    def __init__(self):\n",
        "        self.token_to_id = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.id_to_token = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.vocab_size = 4\n",
        "\n",
        "    def process_text(self, text):\n",
        "        tokens = text.split()\n",
        "        indices = []\n",
        "        for token in tokens:\n",
        "            if token not in self.token_to_id:\n",
        "                self.token_to_id[token] = self.vocab_size\n",
        "                self.id_to_token[self.vocab_size] = token\n",
        "                self.vocab_size += 1\n",
        "            indices.append(self.token_to_id[token])\n",
        "        indices.append(self.token_to_id[\"<EOS>\"])\n",
        "        return indices\n",
        "\n",
        "    def decode_sequence(self, indices):\n",
        "        return \" \".join([self.id_to_token.get(idx, \"<UNK>\") for idx in indices])\n",
        "\n",
        "class TranslationCorpus:\n",
        "    def __init__(self, docx_path):\n",
        "        self.pairs = self._load_translation_pairs(docx_path)\n",
        "        self.source_processor = LanguageProcessor()\n",
        "        self.target_processor = LanguageProcessor()\n",
        "        self._build_vocabularies()\n",
        "\n",
        "    def _load_translation_pairs(self, docx_path):\n",
        "        doc = Document(docx_path)\n",
        "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "        pairs = []\n",
        "        for line in text.split(\"\\n\"):\n",
        "            if '\", \"' in line:\n",
        "                fr, en = line.split('\", \"')\n",
        "                fr = fr.replace('(\"', '').strip()\n",
        "                en = en.replace('\")', '').strip()\n",
        "                pairs.append((fr, en))\n",
        "        return pairs\n",
        "\n",
        "    def _build_vocabularies(self):\n",
        "        for fr, en in self.pairs:\n",
        "            self.source_processor.process_text(fr)\n",
        "            self.target_processor.process_text(en)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = corpus\n",
        "        self.pairs = corpus.pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fr, en = self.pairs[idx]\n",
        "        fr_indices = self.corpus.source_processor.process_text(fr)\n",
        "        en_indices = self.corpus.target_processor.process_text(en)\n",
        "        return torch.tensor(fr_indices), torch.tensor(en_indices)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=100):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Parameter(torch.zeros(max_seq_length, d_model))\n",
        "        nn.init.normal_(self.embedding, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.embedding[:x.size(1), :]\n",
        "\n",
        "class NeuralTranslator(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_layers):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEmbedding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.pos_embed(self.src_embed(src))\n",
        "        tgt_emb = self.pos_embed(self.tgt_embed(tgt))\n",
        "\n",
        "        src_emb = src_emb.transpose(0, 1)\n",
        "        tgt_emb = tgt_emb.transpose(0, 1)\n",
        "\n",
        "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src_emb,\n",
        "            tgt_emb,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "\n",
        "        return self.output_layer(output)\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, learning_rate=0.001):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for src, tgt in self.train_loader:\n",
        "            src, tgt = src.to(self.device), tgt.to(self.device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(src, tgt_input)\n",
        "            output = output.permute(1, 2, 0)\n",
        "\n",
        "            loss = self.criterion(output, tgt_output)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in self.train_loader:\n",
        "                src, tgt = src.to(self.device), tgt.to(self.device)\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_output = tgt[:, 1:]\n",
        "\n",
        "                output = self.model(src, tgt_input)\n",
        "                output = output.permute(1, 2, 0)\n",
        "\n",
        "                loss = self.criterion(output, tgt_output)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                preds = output.argmax(dim=1)\n",
        "                mask = tgt_output != 0\n",
        "                correct += (preds[mask] == tgt_output[mask]).sum().item()\n",
        "                total += mask.sum().item()\n",
        "\n",
        "        return total_loss / len(self.train_loader), correct / total if total > 0 else 0\n",
        "\n",
        "class TranslationGenerator:\n",
        "    def __init__(self, model, source_processor, target_processor):\n",
        "        self.model = model\n",
        "        self.source_processor = source_processor\n",
        "        self.target_processor = target_processor\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def generate(self, source_text, max_length=100):\n",
        "        src_indices = self.source_processor.process_text(source_text)\n",
        "        src_tensor = torch.tensor(src_indices).unsqueeze(0).to(self.device)\n",
        "\n",
        "        tgt_indices = [self.target_processor.token_to_id[\"<SOS>\"]]\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            src_emb = self.model.pos_embed(self.model.src_embed(src_tensor))\n",
        "            src_emb = src_emb.transpose(0, 1)\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(self.device)\n",
        "                tgt_emb = self.model.pos_embed(self.model.tgt_embed(tgt_tensor))\n",
        "                tgt_emb = tgt_emb.transpose(0, 1)\n",
        "\n",
        "                output = self.model.transformer(\n",
        "                    src_emb,\n",
        "                    tgt_emb,\n",
        "                    src_mask=self.model.transformer.generate_square_subsequent_mask(src_tensor.size(1)).to(self.device),\n",
        "                    tgt_mask=self.model.transformer.generate_square_subsequent_mask(tgt_tensor.size(1)).to(self.device)\n",
        "                )\n",
        "\n",
        "                output = self.model.output_layer(output[-1, :, :])\n",
        "                next_token = output.argmax(dim=1).item()\n",
        "                tgt_indices.append(next_token)\n",
        "\n",
        "                if next_token == self.target_processor.token_to_id[\"<EOS>\"]:\n",
        "                    break\n",
        "\n",
        "        return self.target_processor.decode_sequence(tgt_indices[1:-1])\n",
        "\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    corpus = TranslationCorpus(\"Dataset - English to French.docx\")\n",
        "    dataset = TranslationDataset(corpus)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: (\n",
        "        nn.utils.rnn.pad_sequence([item[0] for item in x], batch_first=True, padding_value=0),\n",
        "        nn.utils.rnn.pad_sequence([item[1] for item in x], batch_first=True, padding_value=0)\n",
        "    ))\n",
        "\n",
        "    # Model configurations\n",
        "    configs = {\n",
        "        \"d_model\": 128,\n",
        "        \"num_layers\": [1, 2, 4],\n",
        "        \"nhead\": [2, 4],\n",
        "        \"epochs\": 50,\n",
        "        \"learning_rate\": 0.001\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for num_layers, nhead in product(configs[\"num_layers\"], configs[\"nhead\"]):\n",
        "        print(f\"\\nTransformer: Layers = {num_layers} Heads = {nhead}\")\n",
        "\n",
        "        model = NeuralTranslator(\n",
        "            corpus.source_processor.vocab_size,\n",
        "            corpus.target_processor.vocab_size,\n",
        "            configs[\"d_model\"],\n",
        "            nhead,\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        trainer = ModelTrainer(model, train_loader, configs[\"learning_rate\"])\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(1, configs[\"epochs\"] + 1):\n",
        "            train_loss = trainer.train_epoch()\n",
        "            if epoch % 10 == 0 or epoch == configs[\"epochs\"]:\n",
        "                print(f\"Epoch {epoch}/{configs['epochs']} - Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        exec_time = time.time() - start_time\n",
        "        val_loss, val_acc = trainer.evaluate()\n",
        "        model_size = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        # Qualitative validation\n",
        "        generator = TranslationGenerator(model, corpus.source_processor, corpus.target_processor)\n",
        "        idx = np.random.randint(len(dataset))\n",
        "        fr, en = dataset.pairs[idx]\n",
        "        translated = generator.generate(fr)\n",
        "\n",
        "        print(\"\\nQualitative Validation Example:\")\n",
        "        print(f\"French: {fr}\")\n",
        "        print(f\"True English: {en}\")\n",
        "        print(f\"Predicted English: {translated}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        results.append({\n",
        "            \"Layers\": num_layers,\n",
        "            \"Heads\": nhead,\n",
        "            \"Train Loss\": train_loss,\n",
        "            \"Val Loss\": val_loss,\n",
        "            \"Val Acc\": val_acc,\n",
        "            \"Time\": exec_time,\n",
        "            \"Model Size\": model_size\n",
        "        })\n",
        "\n",
        "    print(\"\\nTransformer Results:\")\n",
        "    for r in results:\n",
        "        print(f\"Layers: {r['Layers']} | Heads: {r['Heads']} | \"\n",
        "              f\"Train Loss: {r['Train Loss']:.4f} | Val Loss: {r['Val Loss']:.4f} | \"\n",
        "              f\"Val Acc: {r['Val Acc']:.4f} | Time: {r['Time']:.2f}s | Model Size: {r['Model Size']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heo3I3QzG49G",
        "outputId": "486d5305-6e3d-49a8-b493-0bdd6da1139c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformer: Layers = 1 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 2.0272\n",
            "Epoch 20/50 - Train Loss: 0.5794\n",
            "Epoch 30/50 - Train Loss: 0.1634\n",
            "Epoch 40/50 - Train Loss: 0.0860\n",
            "Epoch 50/50 - Train Loss: 0.0548\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: The baby cries\n",
            "True English: Le bébé pleure,\n",
            "Predicted English: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 1 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 2.1115\n",
            "Epoch 20/50 - Train Loss: 0.5283\n",
            "Epoch 30/50 - Train Loss: 0.1562\n",
            "Epoch 40/50 - Train Loss: 0.0700\n",
            "Epoch 50/50 - Train Loss: 0.0509\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: She dances at the party\n",
            "True English: Elle danse à la fête,\n",
            "Predicted English: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 2 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 2.0650\n",
            "Epoch 20/50 - Train Loss: 0.5568\n",
            "Epoch 30/50 - Train Loss: 0.1519\n",
            "Epoch 40/50 - Train Loss: 0.0646\n",
            "Epoch 50/50 - Train Loss: 0.0408\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: We love music\n",
            "True English: Nous aimons la musique,\n",
            "Predicted English: à nos grands-parents,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 2 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 2.1791\n",
            "Epoch 20/50 - Train Loss: 0.6015\n",
            "Epoch 30/50 - Train Loss: 0.1515\n",
            "Epoch 40/50 - Train Loss: 0.0622\n",
            "Epoch 50/50 - Train Loss: 0.0391\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: We are friends\n",
            "True English: Nous sommes amis,\n",
            "Predicted English: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 4 Heads = 2\n",
            "Epoch 10/50 - Train Loss: 4.5677\n",
            "Epoch 20/50 - Train Loss: 2.1797\n",
            "Epoch 30/50 - Train Loss: 0.9269\n",
            "Epoch 40/50 - Train Loss: 0.4613\n",
            "Epoch 50/50 - Train Loss: 0.1745\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: She dances gracefully\n",
            "True English: Elle danse avec grâce,\n",
            "Predicted English: des films le vendredi,\n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer: Layers = 4 Heads = 4\n",
            "Epoch 10/50 - Train Loss: 3.3639\n",
            "Epoch 20/50 - Train Loss: 1.3200\n",
            "Epoch 30/50 - Train Loss: 0.5755\n",
            "Epoch 40/50 - Train Loss: 0.2253\n",
            "Epoch 50/50 - Train Loss: 0.0767\n",
            "\n",
            "Qualitative Validation Example:\n",
            "French: We watch a movie together\n",
            "True English: Nous regardons un film ensemble,\n",
            "Predicted English: \n",
            "--------------------------------------------------\n",
            "\n",
            "Transformer Results:\n",
            "Layers: 1 | Heads: 2 | Train Loss: 0.0548 | Val Loss: 0.0178 | Val Acc: 1.0000 | Time: 1.88s | Model Size: 1371551\n",
            "Layers: 1 | Heads: 4 | Train Loss: 0.0509 | Val Loss: 0.0153 | Val Acc: 1.0000 | Time: 1.93s | Model Size: 1371551\n",
            "Layers: 2 | Heads: 2 | Train Loss: 0.0408 | Val Loss: 0.0153 | Val Acc: 1.0000 | Time: 3.05s | Model Size: 2623903\n",
            "Layers: 2 | Heads: 4 | Train Loss: 0.0391 | Val Loss: 0.0149 | Val Acc: 1.0000 | Time: 3.12s | Model Size: 2623903\n",
            "Layers: 4 | Heads: 2 | Train Loss: 0.1745 | Val Loss: 0.0785 | Val Acc: 1.0000 | Time: 5.43s | Model Size: 5128607\n",
            "Layers: 4 | Heads: 4 | Train Loss: 0.0767 | Val Loss: 0.0309 | Val Acc: 1.0000 | Time: 5.36s | Model Size: 5128607\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}